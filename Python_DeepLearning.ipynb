{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Python_DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-dgOuewv1Gt",
        "colab_type": "text"
      },
      "source": [
        "Python에서 Keras를 활용한 딥러닝 강의\n",
        "- Skin cancer classification\n",
        "\n",
        "\n",
        "  실습자료 다운로드: **http://www.snubi.org/~deep/data.zip**\n",
        "\n",
        "\n",
        "Skin melanoma vs Bowen's disease\n",
        "\n",
        "#tensorflow, keras, opencv-python, pillow, scikit-lot, matplotlib 등 라이브러리 설치 필요\n",
        "\n",
        "\n",
        "\n",
        "https://www.python.org/ftp/python/3.6.7/python-3.6.7-amd64.exe\n",
        "\n",
        "- python -m pip install --upgrade pip\n",
        "\n",
        "\n",
        "- pip install ipython\n",
        "\n",
        "- pip install notebook\n",
        "\n",
        "\n",
        "- pip install tensorflow\n",
        "\n",
        "- pip install keras\n",
        "\n",
        "\n",
        "- pip install opencv-python\n",
        "\n",
        "- pip install pillow\n",
        "\n",
        "- pip install scikit-plot\n",
        "\n",
        "- pip install matplotlib\n",
        "\n",
        "- pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArLtF-AZwoGy",
        "colab_type": "code",
        "outputId": "a35ed761-90e1-45f1-f1c0-a08131f15527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "#python -m pip install --upgrade pip\n",
        "#pip install ipython\n",
        "\n",
        "#pip install notebook\n",
        "\n",
        "#pip install tensorflow\n",
        "\n",
        "#pip install keras\n",
        "\n",
        "#pip install opencv-python\n",
        "\n",
        "#pip install pillow\n",
        "\n",
        "!pip install scikit-plot\n",
        "\n",
        "#pip install matplotlib\n",
        "\n",
        "#pip install pandas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-plot\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/47/32520e259340c140a4ad27c1b97050dd3254fdc517b1d59974d47037510e/scikit_plot-0.3.7-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.14.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (3.1.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.22.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (42.0.2)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k33dqOLev1Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Y0iEajfMKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from keras import models, layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.pooling import AveragePooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYDch2f5v1G1",
        "colab_type": "text"
      },
      "source": [
        "*Batch size*는 CPU 혹은 GPU에 한번의 학습에 들어갈 Queue에 이미지의 수.\n",
        "\n",
        "높을수록 효율적이지만, GPU의 메모리를 고려해서 선정해야함.\n",
        "\n",
        "\n",
        "*epoch*는 전체 Training를 한번씩 학습하는 것을 의미한다.\n",
        "\n",
        "따라서 epoch의 수는 전체 데이터를 몇번 학습할지에 대한 횟수를 의미\n",
        "\n",
        "\n",
        "*Learning Rate*는 Back-propagation에서 Error를 줄이는 방향으로 Weight를 수정할 때,\n",
        "\n",
        "얼마나 큰 수정을 가할것이냐에 대한 수치\n",
        "\n",
        "\n",
        "*Decay*는 Learning rate의 update값.\n",
        "\n",
        "학습할수록 점점 작은 Learning rate가 필요.\n",
        "\n",
        "\n",
        "*img_width, img_height는 학습된 Neural network 구조에 동일한 input size가 필요하다.\n",
        "\n",
        "보통 224를 많이 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZKKIfaov1G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(3, input_dim=3, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='relu'))\n",
        "    #Compile model\n",
        "    \n",
        "    model.compile(loss = 'mean_squared_error', optimizer='adam')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDJu-sqZv1G4",
        "colab_type": "text"
      },
      "source": [
        "기본 baseline_model은 입력층으로 3개의 데이터를 받아, 3개 뉴런의 Hidden layer를 가지고,\n",
        "\n",
        "출력층은 하나의 값만을 출력하는 기본 Linear function이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpRfifsZv1G4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.array([[1,2,3], [3,4,1], [2,6,2], [4,3,1], [6,5,4]])\n",
        "y_train = np.array([7, 9, 11, 9, 16])\n",
        "\n",
        "x_test = np.array([[1,1,1], [2,2,2]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm45rhF8v1G7",
        "colab_type": "text"
      },
      "source": [
        "training 데이터와, 결과값을 저장한다.\n",
        "\n",
        "예제로 사용하는 데이터는 y = x+1 함수의 linear function이므로,\n",
        "\n",
        "y_train 값은 x_train의 list에 해당하는 숫자의 합 +1로 구성하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shrd2W6Yv1G7",
        "colab_type": "code",
        "outputId": "b625f116-73b6-4dae-c525-a4880ccee94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = baseline_model()\n",
        "history = model.fit(x_train, y_train, batch_size=1000, epochs = 2000)\n",
        "\n",
        "model.predict(x_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/2000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "5/5 [==============================] - 10s 2s/step - loss: 117.6000\n",
            "Epoch 2/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 3/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 4/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 5/2000\n",
            "5/5 [==============================] - 0s 499us/step - loss: 117.6000\n",
            "Epoch 6/2000\n",
            "5/5 [==============================] - 0s 795us/step - loss: 117.6000\n",
            "Epoch 7/2000\n",
            "5/5 [==============================] - 0s 850us/step - loss: 117.6000\n",
            "Epoch 8/2000\n",
            "5/5 [==============================] - 0s 853us/step - loss: 117.6000\n",
            "Epoch 9/2000\n",
            "5/5 [==============================] - 0s 576us/step - loss: 117.6000\n",
            "Epoch 10/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 11/2000\n",
            "5/5 [==============================] - 0s 620us/step - loss: 117.6000\n",
            "Epoch 12/2000\n",
            "5/5 [==============================] - 0s 462us/step - loss: 117.6000\n",
            "Epoch 13/2000\n",
            "5/5 [==============================] - 0s 543us/step - loss: 117.6000\n",
            "Epoch 14/2000\n",
            "5/5 [==============================] - 0s 500us/step - loss: 117.6000\n",
            "Epoch 15/2000\n",
            "5/5 [==============================] - 0s 720us/step - loss: 117.6000\n",
            "Epoch 16/2000\n",
            "5/5 [==============================] - 0s 606us/step - loss: 117.6000\n",
            "Epoch 17/2000\n",
            "5/5 [==============================] - 0s 676us/step - loss: 117.6000\n",
            "Epoch 18/2000\n",
            "5/5 [==============================] - 0s 585us/step - loss: 117.6000\n",
            "Epoch 19/2000\n",
            "5/5 [==============================] - 0s 572us/step - loss: 117.6000\n",
            "Epoch 20/2000\n",
            "5/5 [==============================] - 0s 727us/step - loss: 117.6000\n",
            "Epoch 21/2000\n",
            "5/5 [==============================] - 0s 736us/step - loss: 117.6000\n",
            "Epoch 22/2000\n",
            "5/5 [==============================] - 0s 553us/step - loss: 117.6000\n",
            "Epoch 23/2000\n",
            "5/5 [==============================] - 0s 549us/step - loss: 117.6000\n",
            "Epoch 24/2000\n",
            "5/5 [==============================] - 0s 533us/step - loss: 117.6000\n",
            "Epoch 25/2000\n",
            "5/5 [==============================] - 0s 581us/step - loss: 117.6000\n",
            "Epoch 26/2000\n",
            "5/5 [==============================] - 0s 591us/step - loss: 117.6000\n",
            "Epoch 27/2000\n",
            "5/5 [==============================] - 0s 566us/step - loss: 117.6000\n",
            "Epoch 28/2000\n",
            "5/5 [==============================] - 0s 625us/step - loss: 117.6000\n",
            "Epoch 29/2000\n",
            "5/5 [==============================] - 0s 776us/step - loss: 117.6000\n",
            "Epoch 30/2000\n",
            "5/5 [==============================] - 0s 514us/step - loss: 117.6000\n",
            "Epoch 31/2000\n",
            "5/5 [==============================] - 0s 547us/step - loss: 117.6000\n",
            "Epoch 32/2000\n",
            "5/5 [==============================] - 0s 559us/step - loss: 117.6000\n",
            "Epoch 33/2000\n",
            "5/5 [==============================] - 0s 526us/step - loss: 117.6000\n",
            "Epoch 34/2000\n",
            "5/5 [==============================] - 0s 615us/step - loss: 117.6000\n",
            "Epoch 35/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 36/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 37/2000\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 117.6000\n",
            "Epoch 38/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 39/2000\n",
            "5/5 [==============================] - 0s 698us/step - loss: 117.6000\n",
            "Epoch 40/2000\n",
            "5/5 [==============================] - 0s 541us/step - loss: 117.6000\n",
            "Epoch 41/2000\n",
            "5/5 [==============================] - 0s 835us/step - loss: 117.6000\n",
            "Epoch 42/2000\n",
            "5/5 [==============================] - 0s 538us/step - loss: 117.6000\n",
            "Epoch 43/2000\n",
            "5/5 [==============================] - 0s 560us/step - loss: 117.6000\n",
            "Epoch 44/2000\n",
            "5/5 [==============================] - 0s 666us/step - loss: 117.6000\n",
            "Epoch 45/2000\n",
            "5/5 [==============================] - 0s 593us/step - loss: 117.6000\n",
            "Epoch 46/2000\n",
            "5/5 [==============================] - 0s 558us/step - loss: 117.6000\n",
            "Epoch 47/2000\n",
            "5/5 [==============================] - 0s 608us/step - loss: 117.6000\n",
            "Epoch 48/2000\n",
            "5/5 [==============================] - 0s 879us/step - loss: 117.6000\n",
            "Epoch 49/2000\n",
            "5/5 [==============================] - 0s 650us/step - loss: 117.6000\n",
            "Epoch 50/2000\n",
            "5/5 [==============================] - 0s 632us/step - loss: 117.6000\n",
            "Epoch 51/2000\n",
            "5/5 [==============================] - 0s 737us/step - loss: 117.6000\n",
            "Epoch 52/2000\n",
            "5/5 [==============================] - 0s 658us/step - loss: 117.6000\n",
            "Epoch 53/2000\n",
            "5/5 [==============================] - 0s 539us/step - loss: 117.6000\n",
            "Epoch 54/2000\n",
            "5/5 [==============================] - 0s 729us/step - loss: 117.6000\n",
            "Epoch 55/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 56/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 57/2000\n",
            "5/5 [==============================] - 0s 905us/step - loss: 117.6000\n",
            "Epoch 58/2000\n",
            "5/5 [==============================] - 0s 662us/step - loss: 117.6000\n",
            "Epoch 59/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 60/2000\n",
            "5/5 [==============================] - 0s 552us/step - loss: 117.6000\n",
            "Epoch 61/2000\n",
            "5/5 [==============================] - 0s 828us/step - loss: 117.6000\n",
            "Epoch 62/2000\n",
            "5/5 [==============================] - 0s 762us/step - loss: 117.6000\n",
            "Epoch 63/2000\n",
            "5/5 [==============================] - 0s 695us/step - loss: 117.6000\n",
            "Epoch 64/2000\n",
            "5/5 [==============================] - 0s 593us/step - loss: 117.6000\n",
            "Epoch 65/2000\n",
            "5/5 [==============================] - 0s 754us/step - loss: 117.6000\n",
            "Epoch 66/2000\n",
            "5/5 [==============================] - 0s 724us/step - loss: 117.6000\n",
            "Epoch 67/2000\n",
            "5/5 [==============================] - 0s 505us/step - loss: 117.6000\n",
            "Epoch 68/2000\n",
            "5/5 [==============================] - 0s 674us/step - loss: 117.6000\n",
            "Epoch 69/2000\n",
            "5/5 [==============================] - 0s 589us/step - loss: 117.6000\n",
            "Epoch 70/2000\n",
            "5/5 [==============================] - 0s 569us/step - loss: 117.6000\n",
            "Epoch 71/2000\n",
            "5/5 [==============================] - 0s 656us/step - loss: 117.6000\n",
            "Epoch 72/2000\n",
            "5/5 [==============================] - 0s 974us/step - loss: 117.6000\n",
            "Epoch 73/2000\n",
            "5/5 [==============================] - 0s 585us/step - loss: 117.6000\n",
            "Epoch 74/2000\n",
            "5/5 [==============================] - 0s 936us/step - loss: 117.6000\n",
            "Epoch 75/2000\n",
            "5/5 [==============================] - 0s 506us/step - loss: 117.6000\n",
            "Epoch 76/2000\n",
            "5/5 [==============================] - 0s 448us/step - loss: 117.6000\n",
            "Epoch 77/2000\n",
            "5/5 [==============================] - 0s 516us/step - loss: 117.6000\n",
            "Epoch 78/2000\n",
            "5/5 [==============================] - 0s 949us/step - loss: 117.6000\n",
            "Epoch 79/2000\n",
            "5/5 [==============================] - 0s 564us/step - loss: 117.6000\n",
            "Epoch 80/2000\n",
            "5/5 [==============================] - 0s 459us/step - loss: 117.6000\n",
            "Epoch 81/2000\n",
            "5/5 [==============================] - 0s 581us/step - loss: 117.6000\n",
            "Epoch 82/2000\n",
            "5/5 [==============================] - 0s 531us/step - loss: 117.6000\n",
            "Epoch 83/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 84/2000\n",
            "5/5 [==============================] - 0s 743us/step - loss: 117.6000\n",
            "Epoch 85/2000\n",
            "5/5 [==============================] - 0s 662us/step - loss: 117.6000\n",
            "Epoch 86/2000\n",
            "5/5 [==============================] - 0s 558us/step - loss: 117.6000\n",
            "Epoch 87/2000\n",
            "5/5 [==============================] - 0s 541us/step - loss: 117.6000\n",
            "Epoch 88/2000\n",
            "5/5 [==============================] - 0s 805us/step - loss: 117.6000\n",
            "Epoch 89/2000\n",
            "5/5 [==============================] - 0s 647us/step - loss: 117.6000\n",
            "Epoch 90/2000\n",
            "5/5 [==============================] - 0s 698us/step - loss: 117.6000\n",
            "Epoch 91/2000\n",
            "5/5 [==============================] - 0s 585us/step - loss: 117.6000\n",
            "Epoch 92/2000\n",
            "5/5 [==============================] - 0s 579us/step - loss: 117.6000\n",
            "Epoch 93/2000\n",
            "5/5 [==============================] - 0s 742us/step - loss: 117.6000\n",
            "Epoch 94/2000\n",
            "5/5 [==============================] - 0s 565us/step - loss: 117.6000\n",
            "Epoch 95/2000\n",
            "5/5 [==============================] - 0s 917us/step - loss: 117.6000\n",
            "Epoch 96/2000\n",
            "5/5 [==============================] - 0s 662us/step - loss: 117.6000\n",
            "Epoch 97/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 98/2000\n",
            "5/5 [==============================] - 0s 891us/step - loss: 117.6000\n",
            "Epoch 99/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 100/2000\n",
            "5/5 [==============================] - 0s 790us/step - loss: 117.6000\n",
            "Epoch 101/2000\n",
            "5/5 [==============================] - 0s 836us/step - loss: 117.6000\n",
            "Epoch 102/2000\n",
            "5/5 [==============================] - 0s 665us/step - loss: 117.6000\n",
            "Epoch 103/2000\n",
            "5/5 [==============================] - 0s 837us/step - loss: 117.6000\n",
            "Epoch 104/2000\n",
            "5/5 [==============================] - 0s 616us/step - loss: 117.6000\n",
            "Epoch 105/2000\n",
            "5/5 [==============================] - 0s 913us/step - loss: 117.6000\n",
            "Epoch 106/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 107/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 108/2000\n",
            "5/5 [==============================] - 0s 868us/step - loss: 117.6000\n",
            "Epoch 109/2000\n",
            "5/5 [==============================] - 0s 646us/step - loss: 117.6000\n",
            "Epoch 110/2000\n",
            "5/5 [==============================] - 0s 604us/step - loss: 117.6000\n",
            "Epoch 111/2000\n",
            "5/5 [==============================] - 0s 548us/step - loss: 117.6000\n",
            "Epoch 112/2000\n",
            "5/5 [==============================] - 0s 504us/step - loss: 117.6000\n",
            "Epoch 113/2000\n",
            "5/5 [==============================] - 0s 507us/step - loss: 117.6000\n",
            "Epoch 114/2000\n",
            "5/5 [==============================] - 0s 517us/step - loss: 117.6000\n",
            "Epoch 115/2000\n",
            "5/5 [==============================] - 0s 673us/step - loss: 117.6000\n",
            "Epoch 116/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 117/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 118/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 119/2000\n",
            "5/5 [==============================] - 0s 887us/step - loss: 117.6000\n",
            "Epoch 120/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 121/2000\n",
            "5/5 [==============================] - 0s 941us/step - loss: 117.6000\n",
            "Epoch 122/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 123/2000\n",
            "5/5 [==============================] - 0s 976us/step - loss: 117.6000\n",
            "Epoch 124/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 125/2000\n",
            "5/5 [==============================] - 0s 991us/step - loss: 117.6000\n",
            "Epoch 126/2000\n",
            "5/5 [==============================] - 0s 871us/step - loss: 117.6000\n",
            "Epoch 127/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 128/2000\n",
            "5/5 [==============================] - 0s 834us/step - loss: 117.6000\n",
            "Epoch 129/2000\n",
            "5/5 [==============================] - 0s 753us/step - loss: 117.6000\n",
            "Epoch 130/2000\n",
            "5/5 [==============================] - 0s 879us/step - loss: 117.6000\n",
            "Epoch 131/2000\n",
            "5/5 [==============================] - 0s 444us/step - loss: 117.6000\n",
            "Epoch 132/2000\n",
            "5/5 [==============================] - 0s 696us/step - loss: 117.6000\n",
            "Epoch 133/2000\n",
            "5/5 [==============================] - 0s 448us/step - loss: 117.6000\n",
            "Epoch 134/2000\n",
            "5/5 [==============================] - 0s 719us/step - loss: 117.6000\n",
            "Epoch 135/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 136/2000\n",
            "5/5 [==============================] - 0s 459us/step - loss: 117.6000\n",
            "Epoch 137/2000\n",
            "5/5 [==============================] - 0s 706us/step - loss: 117.6000\n",
            "Epoch 138/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 139/2000\n",
            "5/5 [==============================] - 0s 794us/step - loss: 117.6000\n",
            "Epoch 140/2000\n",
            "5/5 [==============================] - 0s 585us/step - loss: 117.6000\n",
            "Epoch 141/2000\n",
            "5/5 [==============================] - 0s 740us/step - loss: 117.6000\n",
            "Epoch 142/2000\n",
            "5/5 [==============================] - 0s 535us/step - loss: 117.6000\n",
            "Epoch 143/2000\n",
            "5/5 [==============================] - 0s 607us/step - loss: 117.6000\n",
            "Epoch 144/2000\n",
            "5/5 [==============================] - 0s 619us/step - loss: 117.6000\n",
            "Epoch 145/2000\n",
            "5/5 [==============================] - 0s 619us/step - loss: 117.6000\n",
            "Epoch 146/2000\n",
            "5/5 [==============================] - 0s 735us/step - loss: 117.6000\n",
            "Epoch 147/2000\n",
            "5/5 [==============================] - 0s 618us/step - loss: 117.6000\n",
            "Epoch 148/2000\n",
            "5/5 [==============================] - 0s 825us/step - loss: 117.6000\n",
            "Epoch 149/2000\n",
            "5/5 [==============================] - 0s 653us/step - loss: 117.6000\n",
            "Epoch 150/2000\n",
            "5/5 [==============================] - 0s 810us/step - loss: 117.6000\n",
            "Epoch 151/2000\n",
            "5/5 [==============================] - 0s 873us/step - loss: 117.6000\n",
            "Epoch 152/2000\n",
            "5/5 [==============================] - 0s 608us/step - loss: 117.6000\n",
            "Epoch 153/2000\n",
            "5/5 [==============================] - 0s 781us/step - loss: 117.6000\n",
            "Epoch 154/2000\n",
            "5/5 [==============================] - 0s 819us/step - loss: 117.6000\n",
            "Epoch 155/2000\n",
            "5/5 [==============================] - 0s 653us/step - loss: 117.6000\n",
            "Epoch 156/2000\n",
            "5/5 [==============================] - 0s 809us/step - loss: 117.6000\n",
            "Epoch 157/2000\n",
            "5/5 [==============================] - 0s 748us/step - loss: 117.6000\n",
            "Epoch 158/2000\n",
            "5/5 [==============================] - 0s 767us/step - loss: 117.6000\n",
            "Epoch 159/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 160/2000\n",
            "5/5 [==============================] - 0s 701us/step - loss: 117.6000\n",
            "Epoch 161/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 162/2000\n",
            "5/5 [==============================] - 0s 501us/step - loss: 117.6000\n",
            "Epoch 163/2000\n",
            "5/5 [==============================] - 0s 982us/step - loss: 117.6000\n",
            "Epoch 164/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 165/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 166/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 167/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 168/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 169/2000\n",
            "5/5 [==============================] - 0s 999us/step - loss: 117.6000\n",
            "Epoch 170/2000\n",
            "5/5 [==============================] - 0s 992us/step - loss: 117.6000\n",
            "Epoch 171/2000\n",
            "5/5 [==============================] - 0s 752us/step - loss: 117.6000\n",
            "Epoch 172/2000\n",
            "5/5 [==============================] - 0s 633us/step - loss: 117.6000\n",
            "Epoch 173/2000\n",
            "5/5 [==============================] - 0s 677us/step - loss: 117.6000\n",
            "Epoch 174/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 175/2000\n",
            "5/5 [==============================] - 0s 642us/step - loss: 117.6000\n",
            "Epoch 176/2000\n",
            "5/5 [==============================] - 0s 858us/step - loss: 117.6000\n",
            "Epoch 177/2000\n",
            "5/5 [==============================] - 0s 474us/step - loss: 117.6000\n",
            "Epoch 178/2000\n",
            "5/5 [==============================] - 0s 594us/step - loss: 117.6000\n",
            "Epoch 179/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 180/2000\n",
            "5/5 [==============================] - 0s 483us/step - loss: 117.6000\n",
            "Epoch 181/2000\n",
            "5/5 [==============================] - 0s 476us/step - loss: 117.6000\n",
            "Epoch 182/2000\n",
            "5/5 [==============================] - 0s 601us/step - loss: 117.6000\n",
            "Epoch 183/2000\n",
            "5/5 [==============================] - 0s 812us/step - loss: 117.6000\n",
            "Epoch 184/2000\n",
            "5/5 [==============================] - 0s 609us/step - loss: 117.6000\n",
            "Epoch 185/2000\n",
            "5/5 [==============================] - 0s 618us/step - loss: 117.6000\n",
            "Epoch 186/2000\n",
            "5/5 [==============================] - 0s 427us/step - loss: 117.6000\n",
            "Epoch 187/2000\n",
            "5/5 [==============================] - 0s 683us/step - loss: 117.6000\n",
            "Epoch 188/2000\n",
            "5/5 [==============================] - 0s 419us/step - loss: 117.6000\n",
            "Epoch 189/2000\n",
            "5/5 [==============================] - 0s 506us/step - loss: 117.6000\n",
            "Epoch 190/2000\n",
            "5/5 [==============================] - 0s 519us/step - loss: 117.6000\n",
            "Epoch 191/2000\n",
            "5/5 [==============================] - 0s 485us/step - loss: 117.6000\n",
            "Epoch 192/2000\n",
            "5/5 [==============================] - 0s 524us/step - loss: 117.6000\n",
            "Epoch 193/2000\n",
            "5/5 [==============================] - 0s 534us/step - loss: 117.6000\n",
            "Epoch 194/2000\n",
            "5/5 [==============================] - 0s 940us/step - loss: 117.6000\n",
            "Epoch 195/2000\n",
            "5/5 [==============================] - 0s 606us/step - loss: 117.6000\n",
            "Epoch 196/2000\n",
            "5/5 [==============================] - 0s 973us/step - loss: 117.6000\n",
            "Epoch 197/2000\n",
            "5/5 [==============================] - 0s 842us/step - loss: 117.6000\n",
            "Epoch 198/2000\n",
            "5/5 [==============================] - 0s 610us/step - loss: 117.6000\n",
            "Epoch 199/2000\n",
            "5/5 [==============================] - 0s 631us/step - loss: 117.6000\n",
            "Epoch 200/2000\n",
            "5/5 [==============================] - 0s 751us/step - loss: 117.6000\n",
            "Epoch 201/2000\n",
            "5/5 [==============================] - 0s 733us/step - loss: 117.6000\n",
            "Epoch 202/2000\n",
            "5/5 [==============================] - 0s 696us/step - loss: 117.6000\n",
            "Epoch 203/2000\n",
            "5/5 [==============================] - 0s 689us/step - loss: 117.6000\n",
            "Epoch 204/2000\n",
            "5/5 [==============================] - 0s 821us/step - loss: 117.6000\n",
            "Epoch 205/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 206/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 207/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 208/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 209/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 210/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 211/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 212/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 213/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 214/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 215/2000\n",
            "5/5 [==============================] - 0s 793us/step - loss: 117.6000\n",
            "Epoch 216/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 217/2000\n",
            "5/5 [==============================] - 0s 873us/step - loss: 117.6000\n",
            "Epoch 218/2000\n",
            "5/5 [==============================] - 0s 853us/step - loss: 117.6000\n",
            "Epoch 219/2000\n",
            "5/5 [==============================] - 0s 847us/step - loss: 117.6000\n",
            "Epoch 220/2000\n",
            "5/5 [==============================] - 0s 939us/step - loss: 117.6000\n",
            "Epoch 221/2000\n",
            "5/5 [==============================] - 0s 815us/step - loss: 117.6000\n",
            "Epoch 222/2000\n",
            "5/5 [==============================] - 0s 811us/step - loss: 117.6000\n",
            "Epoch 223/2000\n",
            "5/5 [==============================] - 0s 953us/step - loss: 117.6000\n",
            "Epoch 224/2000\n",
            "5/5 [==============================] - 0s 702us/step - loss: 117.6000\n",
            "Epoch 225/2000\n",
            "5/5 [==============================] - 0s 758us/step - loss: 117.6000\n",
            "Epoch 226/2000\n",
            "5/5 [==============================] - 0s 802us/step - loss: 117.6000\n",
            "Epoch 227/2000\n",
            "5/5 [==============================] - 0s 810us/step - loss: 117.6000\n",
            "Epoch 228/2000\n",
            "5/5 [==============================] - 0s 734us/step - loss: 117.6000\n",
            "Epoch 229/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 230/2000\n",
            "5/5 [==============================] - 0s 887us/step - loss: 117.6000\n",
            "Epoch 231/2000\n",
            "5/5 [==============================] - 0s 808us/step - loss: 117.6000\n",
            "Epoch 232/2000\n",
            "5/5 [==============================] - 0s 902us/step - loss: 117.6000\n",
            "Epoch 233/2000\n",
            "5/5 [==============================] - 0s 955us/step - loss: 117.6000\n",
            "Epoch 234/2000\n",
            "5/5 [==============================] - 0s 959us/step - loss: 117.6000\n",
            "Epoch 235/2000\n",
            "5/5 [==============================] - 0s 862us/step - loss: 117.6000\n",
            "Epoch 236/2000\n",
            "5/5 [==============================] - 0s 748us/step - loss: 117.6000\n",
            "Epoch 237/2000\n",
            "5/5 [==============================] - 0s 832us/step - loss: 117.6000\n",
            "Epoch 238/2000\n",
            "5/5 [==============================] - 0s 748us/step - loss: 117.6000\n",
            "Epoch 239/2000\n",
            "5/5 [==============================] - 0s 956us/step - loss: 117.6000\n",
            "Epoch 240/2000\n",
            "5/5 [==============================] - 0s 715us/step - loss: 117.6000\n",
            "Epoch 241/2000\n",
            "5/5 [==============================] - 0s 984us/step - loss: 117.6000\n",
            "Epoch 242/2000\n",
            "5/5 [==============================] - 0s 988us/step - loss: 117.6000\n",
            "Epoch 243/2000\n",
            "5/5 [==============================] - 0s 714us/step - loss: 117.6000\n",
            "Epoch 244/2000\n",
            "5/5 [==============================] - 0s 810us/step - loss: 117.6000\n",
            "Epoch 245/2000\n",
            "5/5 [==============================] - 0s 770us/step - loss: 117.6000\n",
            "Epoch 246/2000\n",
            "5/5 [==============================] - 0s 999us/step - loss: 117.6000\n",
            "Epoch 247/2000\n",
            "5/5 [==============================] - 0s 853us/step - loss: 117.6000\n",
            "Epoch 248/2000\n",
            "5/5 [==============================] - 0s 818us/step - loss: 117.6000\n",
            "Epoch 249/2000\n",
            "5/5 [==============================] - 0s 743us/step - loss: 117.6000\n",
            "Epoch 250/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 251/2000\n",
            "5/5 [==============================] - 0s 776us/step - loss: 117.6000\n",
            "Epoch 252/2000\n",
            "5/5 [==============================] - 0s 831us/step - loss: 117.6000\n",
            "Epoch 253/2000\n",
            "5/5 [==============================] - 0s 782us/step - loss: 117.6000\n",
            "Epoch 254/2000\n",
            "5/5 [==============================] - 0s 778us/step - loss: 117.6000\n",
            "Epoch 255/2000\n",
            "5/5 [==============================] - 0s 758us/step - loss: 117.6000\n",
            "Epoch 256/2000\n",
            "5/5 [==============================] - 0s 982us/step - loss: 117.6000\n",
            "Epoch 257/2000\n",
            "5/5 [==============================] - 0s 744us/step - loss: 117.6000\n",
            "Epoch 258/2000\n",
            "5/5 [==============================] - 0s 710us/step - loss: 117.6000\n",
            "Epoch 259/2000\n",
            "5/5 [==============================] - 0s 466us/step - loss: 117.6000\n",
            "Epoch 260/2000\n",
            "5/5 [==============================] - 0s 489us/step - loss: 117.6000\n",
            "Epoch 261/2000\n",
            "5/5 [==============================] - 0s 782us/step - loss: 117.6000\n",
            "Epoch 262/2000\n",
            "5/5 [==============================] - 0s 642us/step - loss: 117.6000\n",
            "Epoch 263/2000\n",
            "5/5 [==============================] - 0s 592us/step - loss: 117.6000\n",
            "Epoch 264/2000\n",
            "5/5 [==============================] - 0s 898us/step - loss: 117.6000\n",
            "Epoch 265/2000\n",
            "5/5 [==============================] - 0s 707us/step - loss: 117.6000\n",
            "Epoch 266/2000\n",
            "5/5 [==============================] - 0s 422us/step - loss: 117.6000\n",
            "Epoch 267/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 268/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 269/2000\n",
            "5/5 [==============================] - 0s 959us/step - loss: 117.6000\n",
            "Epoch 270/2000\n",
            "5/5 [==============================] - 0s 941us/step - loss: 117.6000\n",
            "Epoch 271/2000\n",
            "5/5 [==============================] - 0s 848us/step - loss: 117.6000\n",
            "Epoch 272/2000\n",
            "5/5 [==============================] - 0s 960us/step - loss: 117.6000\n",
            "Epoch 273/2000\n",
            "5/5 [==============================] - 0s 506us/step - loss: 117.6000\n",
            "Epoch 274/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 275/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 276/2000\n",
            "5/5 [==============================] - 0s 948us/step - loss: 117.6000\n",
            "Epoch 277/2000\n",
            "5/5 [==============================] - 0s 876us/step - loss: 117.6000\n",
            "Epoch 278/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 279/2000\n",
            "5/5 [==============================] - 0s 501us/step - loss: 117.6000\n",
            "Epoch 280/2000\n",
            "5/5 [==============================] - 0s 745us/step - loss: 117.6000\n",
            "Epoch 281/2000\n",
            "5/5 [==============================] - 0s 415us/step - loss: 117.6000\n",
            "Epoch 282/2000\n",
            "5/5 [==============================] - 0s 467us/step - loss: 117.6000\n",
            "Epoch 283/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 284/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 285/2000\n",
            "5/5 [==============================] - 0s 916us/step - loss: 117.6000\n",
            "Epoch 286/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 287/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 288/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 289/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 290/2000\n",
            "5/5 [==============================] - 0s 980us/step - loss: 117.6000\n",
            "Epoch 291/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 292/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 293/2000\n",
            "5/5 [==============================] - 0s 964us/step - loss: 117.6000\n",
            "Epoch 294/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 295/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 296/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 297/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 298/2000\n",
            "5/5 [==============================] - 0s 798us/step - loss: 117.6000\n",
            "Epoch 299/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 300/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 301/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 302/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 303/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 304/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 305/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 306/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 307/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 308/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 309/2000\n",
            "5/5 [==============================] - 0s 721us/step - loss: 117.6000\n",
            "Epoch 310/2000\n",
            "5/5 [==============================] - 0s 769us/step - loss: 117.6000\n",
            "Epoch 311/2000\n",
            "5/5 [==============================] - 0s 855us/step - loss: 117.6000\n",
            "Epoch 312/2000\n",
            "5/5 [==============================] - 0s 569us/step - loss: 117.6000\n",
            "Epoch 313/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 314/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 315/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 316/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 317/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 318/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 319/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 320/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 321/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 322/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 323/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 324/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 325/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 326/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 327/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 328/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 329/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 330/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 331/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 332/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 333/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 334/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 335/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 336/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 337/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 338/2000\n",
            "5/5 [==============================] - 0s 821us/step - loss: 117.6000\n",
            "Epoch 339/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 340/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 341/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 342/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 343/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 344/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 345/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 346/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 347/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 348/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 349/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 350/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 351/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 352/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 353/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 354/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 355/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 356/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 357/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 358/2000\n",
            "5/5 [==============================] - 0s 908us/step - loss: 117.6000\n",
            "Epoch 359/2000\n",
            "5/5 [==============================] - 0s 979us/step - loss: 117.6000\n",
            "Epoch 360/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 361/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 362/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 363/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 364/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 365/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 366/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 367/2000\n",
            "5/5 [==============================] - 0s 758us/step - loss: 117.6000\n",
            "Epoch 368/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 369/2000\n",
            "5/5 [==============================] - 0s 843us/step - loss: 117.6000\n",
            "Epoch 370/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 371/2000\n",
            "5/5 [==============================] - 0s 924us/step - loss: 117.6000\n",
            "Epoch 372/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 373/2000\n",
            "5/5 [==============================] - 0s 857us/step - loss: 117.6000\n",
            "Epoch 374/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 375/2000\n",
            "5/5 [==============================] - 0s 906us/step - loss: 117.6000\n",
            "Epoch 376/2000\n",
            "5/5 [==============================] - 0s 690us/step - loss: 117.6000\n",
            "Epoch 377/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 378/2000\n",
            "5/5 [==============================] - 0s 555us/step - loss: 117.6000\n",
            "Epoch 379/2000\n",
            "5/5 [==============================] - 0s 756us/step - loss: 117.6000\n",
            "Epoch 380/2000\n",
            "5/5 [==============================] - 0s 711us/step - loss: 117.6000\n",
            "Epoch 381/2000\n",
            "5/5 [==============================] - 0s 673us/step - loss: 117.6000\n",
            "Epoch 382/2000\n",
            "5/5 [==============================] - 0s 831us/step - loss: 117.6000\n",
            "Epoch 383/2000\n",
            "5/5 [==============================] - 0s 754us/step - loss: 117.6000\n",
            "Epoch 384/2000\n",
            "5/5 [==============================] - 0s 611us/step - loss: 117.6000\n",
            "Epoch 385/2000\n",
            "5/5 [==============================] - 0s 637us/step - loss: 117.6000\n",
            "Epoch 386/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 387/2000\n",
            "5/5 [==============================] - 0s 943us/step - loss: 117.6000\n",
            "Epoch 388/2000\n",
            "5/5 [==============================] - 0s 975us/step - loss: 117.6000\n",
            "Epoch 389/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 390/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 391/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 392/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 393/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 394/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 395/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 396/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 397/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 398/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 399/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 400/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 401/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 402/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 403/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 404/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 405/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 406/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 407/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 408/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 409/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 410/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 411/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 412/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 413/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 414/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 415/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 416/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 417/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 418/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 419/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 420/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 421/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 422/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 423/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 424/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 425/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 426/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 427/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 428/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 429/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 430/2000\n",
            "5/5 [==============================] - 0s 981us/step - loss: 117.6000\n",
            "Epoch 431/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 432/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 433/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 434/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 435/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 436/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 437/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 438/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 439/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 440/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 441/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 442/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 443/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 444/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 445/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 446/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 447/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 448/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 449/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 450/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 451/2000\n",
            "5/5 [==============================] - 0s 931us/step - loss: 117.6000\n",
            "Epoch 452/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 453/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 454/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 455/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 456/2000\n",
            "5/5 [==============================] - 0s 961us/step - loss: 117.6000\n",
            "Epoch 457/2000\n",
            "5/5 [==============================] - 0s 996us/step - loss: 117.6000\n",
            "Epoch 458/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 459/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 460/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 461/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 462/2000\n",
            "5/5 [==============================] - 0s 946us/step - loss: 117.6000\n",
            "Epoch 463/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 464/2000\n",
            "5/5 [==============================] - 0s 996us/step - loss: 117.6000\n",
            "Epoch 465/2000\n",
            "5/5 [==============================] - 0s 922us/step - loss: 117.6000\n",
            "Epoch 466/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 467/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 468/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 469/2000\n",
            "5/5 [==============================] - 0s 944us/step - loss: 117.6000\n",
            "Epoch 470/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 471/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 472/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 473/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 474/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 475/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 476/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 477/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 478/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 479/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 480/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 481/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 482/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 483/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 484/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 485/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 486/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 487/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 488/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 489/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 490/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 491/2000\n",
            "5/5 [==============================] - 0s 999us/step - loss: 117.6000\n",
            "Epoch 492/2000\n",
            "5/5 [==============================] - 0s 938us/step - loss: 117.6000\n",
            "Epoch 493/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 494/2000\n",
            "5/5 [==============================] - 0s 998us/step - loss: 117.6000\n",
            "Epoch 495/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 496/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 497/2000\n",
            "5/5 [==============================] - 0s 892us/step - loss: 117.6000\n",
            "Epoch 498/2000\n",
            "5/5 [==============================] - 0s 941us/step - loss: 117.6000\n",
            "Epoch 499/2000\n",
            "5/5 [==============================] - 0s 961us/step - loss: 117.6000\n",
            "Epoch 500/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 501/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 502/2000\n",
            "5/5 [==============================] - 0s 982us/step - loss: 117.6000\n",
            "Epoch 503/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 504/2000\n",
            "5/5 [==============================] - 0s 961us/step - loss: 117.6000\n",
            "Epoch 505/2000\n",
            "5/5 [==============================] - 0s 967us/step - loss: 117.6000\n",
            "Epoch 506/2000\n",
            "5/5 [==============================] - 0s 955us/step - loss: 117.6000\n",
            "Epoch 507/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 508/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 509/2000\n",
            "5/5 [==============================] - 0s 903us/step - loss: 117.6000\n",
            "Epoch 510/2000\n",
            "5/5 [==============================] - 0s 991us/step - loss: 117.6000\n",
            "Epoch 511/2000\n",
            "5/5 [==============================] - 0s 936us/step - loss: 117.6000\n",
            "Epoch 512/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 513/2000\n",
            "5/5 [==============================] - 0s 991us/step - loss: 117.6000\n",
            "Epoch 514/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 515/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 516/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 517/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 518/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 519/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 520/2000\n",
            "5/5 [==============================] - 0s 945us/step - loss: 117.6000\n",
            "Epoch 521/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 522/2000\n",
            "5/5 [==============================] - 0s 950us/step - loss: 117.6000\n",
            "Epoch 523/2000\n",
            "5/5 [==============================] - 0s 969us/step - loss: 117.6000\n",
            "Epoch 524/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 525/2000\n",
            "5/5 [==============================] - 0s 977us/step - loss: 117.6000\n",
            "Epoch 526/2000\n",
            "5/5 [==============================] - 0s 986us/step - loss: 117.6000\n",
            "Epoch 527/2000\n",
            "5/5 [==============================] - 0s 984us/step - loss: 117.6000\n",
            "Epoch 528/2000\n",
            "5/5 [==============================] - 0s 880us/step - loss: 117.6000\n",
            "Epoch 529/2000\n",
            "5/5 [==============================] - 0s 839us/step - loss: 117.6000\n",
            "Epoch 530/2000\n",
            "5/5 [==============================] - 0s 829us/step - loss: 117.6000\n",
            "Epoch 531/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 532/2000\n",
            "5/5 [==============================] - 0s 861us/step - loss: 117.6000\n",
            "Epoch 533/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 534/2000\n",
            "5/5 [==============================] - 0s 971us/step - loss: 117.6000\n",
            "Epoch 535/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 536/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 537/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 538/2000\n",
            "5/5 [==============================] - 0s 849us/step - loss: 117.6000\n",
            "Epoch 539/2000\n",
            "5/5 [==============================] - 0s 593us/step - loss: 117.6000\n",
            "Epoch 540/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 541/2000\n",
            "5/5 [==============================] - 0s 900us/step - loss: 117.6000\n",
            "Epoch 542/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 543/2000\n",
            "5/5 [==============================] - 0s 884us/step - loss: 117.6000\n",
            "Epoch 544/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 545/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 546/2000\n",
            "5/5 [==============================] - 0s 982us/step - loss: 117.6000\n",
            "Epoch 547/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 548/2000\n",
            "5/5 [==============================] - 0s 972us/step - loss: 117.6000\n",
            "Epoch 549/2000\n",
            "5/5 [==============================] - 0s 652us/step - loss: 117.6000\n",
            "Epoch 550/2000\n",
            "5/5 [==============================] - 0s 697us/step - loss: 117.6000\n",
            "Epoch 551/2000\n",
            "5/5 [==============================] - 0s 831us/step - loss: 117.6000\n",
            "Epoch 552/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 553/2000\n",
            "5/5 [==============================] - 0s 686us/step - loss: 117.6000\n",
            "Epoch 554/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 555/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 556/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 557/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 558/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 559/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 560/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 561/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 562/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 563/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 564/2000\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 117.6000\n",
            "Epoch 565/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 566/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 567/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 568/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 569/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 570/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 571/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 572/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 573/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 574/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 575/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 576/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 577/2000\n",
            "5/5 [==============================] - 0s 885us/step - loss: 117.6000\n",
            "Epoch 578/2000\n",
            "5/5 [==============================] - 0s 587us/step - loss: 117.6000\n",
            "Epoch 579/2000\n",
            "5/5 [==============================] - 0s 791us/step - loss: 117.6000\n",
            "Epoch 580/2000\n",
            "5/5 [==============================] - 0s 921us/step - loss: 117.6000\n",
            "Epoch 581/2000\n",
            "5/5 [==============================] - 0s 546us/step - loss: 117.6000\n",
            "Epoch 582/2000\n",
            "5/5 [==============================] - 0s 626us/step - loss: 117.6000\n",
            "Epoch 583/2000\n",
            "5/5 [==============================] - 0s 603us/step - loss: 117.6000\n",
            "Epoch 584/2000\n",
            "5/5 [==============================] - 0s 852us/step - loss: 117.6000\n",
            "Epoch 585/2000\n",
            "5/5 [==============================] - 0s 627us/step - loss: 117.6000\n",
            "Epoch 586/2000\n",
            "5/5 [==============================] - 0s 574us/step - loss: 117.6000\n",
            "Epoch 587/2000\n",
            "5/5 [==============================] - 0s 506us/step - loss: 117.6000\n",
            "Epoch 588/2000\n",
            "5/5 [==============================] - 0s 589us/step - loss: 117.6000\n",
            "Epoch 589/2000\n",
            "5/5 [==============================] - 0s 566us/step - loss: 117.6000\n",
            "Epoch 590/2000\n",
            "5/5 [==============================] - 0s 553us/step - loss: 117.6000\n",
            "Epoch 591/2000\n",
            "5/5 [==============================] - 0s 565us/step - loss: 117.6000\n",
            "Epoch 592/2000\n",
            "5/5 [==============================] - 0s 556us/step - loss: 117.6000\n",
            "Epoch 593/2000\n",
            "5/5 [==============================] - 0s 544us/step - loss: 117.6000\n",
            "Epoch 594/2000\n",
            "5/5 [==============================] - 0s 628us/step - loss: 117.6000\n",
            "Epoch 595/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 596/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 597/2000\n",
            "5/5 [==============================] - 0s 705us/step - loss: 117.6000\n",
            "Epoch 598/2000\n",
            "5/5 [==============================] - 0s 995us/step - loss: 117.6000\n",
            "Epoch 599/2000\n",
            "5/5 [==============================] - 0s 606us/step - loss: 117.6000\n",
            "Epoch 600/2000\n",
            "5/5 [==============================] - 0s 573us/step - loss: 117.6000\n",
            "Epoch 601/2000\n",
            "5/5 [==============================] - 0s 596us/step - loss: 117.6000\n",
            "Epoch 602/2000\n",
            "5/5 [==============================] - 0s 641us/step - loss: 117.6000\n",
            "Epoch 603/2000\n",
            "5/5 [==============================] - 0s 739us/step - loss: 117.6000\n",
            "Epoch 604/2000\n",
            "5/5 [==============================] - 0s 764us/step - loss: 117.6000\n",
            "Epoch 605/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 606/2000\n",
            "5/5 [==============================] - 0s 679us/step - loss: 117.6000\n",
            "Epoch 607/2000\n",
            "5/5 [==============================] - 0s 671us/step - loss: 117.6000\n",
            "Epoch 608/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 609/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 610/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 611/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 612/2000\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 117.6000\n",
            "Epoch 613/2000\n",
            "5/5 [==============================] - 0s 835us/step - loss: 117.6000\n",
            "Epoch 614/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 615/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 616/2000\n",
            "5/5 [==============================] - 0s 816us/step - loss: 117.6000\n",
            "Epoch 617/2000\n",
            "5/5 [==============================] - 0s 793us/step - loss: 117.6000\n",
            "Epoch 618/2000\n",
            "5/5 [==============================] - 0s 694us/step - loss: 117.6000\n",
            "Epoch 619/2000\n",
            "5/5 [==============================] - 0s 801us/step - loss: 117.6000\n",
            "Epoch 620/2000\n",
            "5/5 [==============================] - 0s 650us/step - loss: 117.6000\n",
            "Epoch 621/2000\n",
            "5/5 [==============================] - 0s 832us/step - loss: 117.6000\n",
            "Epoch 622/2000\n",
            "5/5 [==============================] - 0s 724us/step - loss: 117.6000\n",
            "Epoch 623/2000\n",
            "5/5 [==============================] - 0s 860us/step - loss: 117.6000\n",
            "Epoch 624/2000\n",
            "5/5 [==============================] - 0s 692us/step - loss: 117.6000\n",
            "Epoch 625/2000\n",
            "5/5 [==============================] - 0s 821us/step - loss: 117.6000\n",
            "Epoch 626/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 627/2000\n",
            "5/5 [==============================] - 0s 654us/step - loss: 117.6000\n",
            "Epoch 628/2000\n",
            "5/5 [==============================] - 0s 504us/step - loss: 117.6000\n",
            "Epoch 629/2000\n",
            "5/5 [==============================] - 0s 994us/step - loss: 117.6000\n",
            "Epoch 630/2000\n",
            "5/5 [==============================] - 0s 585us/step - loss: 117.6000\n",
            "Epoch 631/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 632/2000\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 117.6000\n",
            "Epoch 633/2000\n",
            "5/5 [==============================] - 0s 985us/step - loss: 117.6000\n",
            "Epoch 634/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 635/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 636/2000\n",
            "5/5 [==============================] - 0s 743us/step - loss: 117.6000\n",
            "Epoch 637/2000\n",
            "5/5 [==============================] - 0s 870us/step - loss: 117.6000\n",
            "Epoch 638/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 639/2000\n",
            "5/5 [==============================] - 0s 691us/step - loss: 117.6000\n",
            "Epoch 640/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 641/2000\n",
            "5/5 [==============================] - 0s 720us/step - loss: 117.6000\n",
            "Epoch 642/2000\n",
            "5/5 [==============================] - 0s 935us/step - loss: 117.6000\n",
            "Epoch 643/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 644/2000\n",
            "5/5 [==============================] - 0s 716us/step - loss: 117.6000\n",
            "Epoch 645/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 646/2000\n",
            "5/5 [==============================] - 0s 781us/step - loss: 117.6000\n",
            "Epoch 647/2000\n",
            "5/5 [==============================] - 0s 914us/step - loss: 117.6000\n",
            "Epoch 648/2000\n",
            "5/5 [==============================] - 0s 643us/step - loss: 117.6000\n",
            "Epoch 649/2000\n",
            "5/5 [==============================] - 0s 760us/step - loss: 117.6000\n",
            "Epoch 650/2000\n",
            "5/5 [==============================] - 0s 846us/step - loss: 117.6000\n",
            "Epoch 651/2000\n",
            "5/5 [==============================] - 0s 752us/step - loss: 117.6000\n",
            "Epoch 652/2000\n",
            "5/5 [==============================] - 0s 968us/step - loss: 117.6000\n",
            "Epoch 653/2000\n",
            "5/5 [==============================] - 0s 673us/step - loss: 117.6000\n",
            "Epoch 654/2000\n",
            "5/5 [==============================] - 0s 798us/step - loss: 117.6000\n",
            "Epoch 655/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 656/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 657/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 658/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 659/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 660/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 661/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 662/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 663/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 664/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 665/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 666/2000\n",
            "5/5 [==============================] - 0s 531us/step - loss: 117.6000\n",
            "Epoch 667/2000\n",
            "5/5 [==============================] - 0s 687us/step - loss: 117.6000\n",
            "Epoch 668/2000\n",
            "5/5 [==============================] - 0s 488us/step - loss: 117.6000\n",
            "Epoch 669/2000\n",
            "5/5 [==============================] - 0s 741us/step - loss: 117.6000\n",
            "Epoch 670/2000\n",
            "5/5 [==============================] - 0s 627us/step - loss: 117.6000\n",
            "Epoch 671/2000\n",
            "5/5 [==============================] - 0s 615us/step - loss: 117.6000\n",
            "Epoch 672/2000\n",
            "5/5 [==============================] - 0s 675us/step - loss: 117.6000\n",
            "Epoch 673/2000\n",
            "5/5 [==============================] - 0s 557us/step - loss: 117.6000\n",
            "Epoch 674/2000\n",
            "5/5 [==============================] - 0s 771us/step - loss: 117.6000\n",
            "Epoch 675/2000\n",
            "5/5 [==============================] - 0s 540us/step - loss: 117.6000\n",
            "Epoch 676/2000\n",
            "5/5 [==============================] - 0s 482us/step - loss: 117.6000\n",
            "Epoch 677/2000\n",
            "5/5 [==============================] - 0s 510us/step - loss: 117.6000\n",
            "Epoch 678/2000\n",
            "5/5 [==============================] - 0s 509us/step - loss: 117.6000\n",
            "Epoch 679/2000\n",
            "5/5 [==============================] - 0s 535us/step - loss: 117.6000\n",
            "Epoch 680/2000\n",
            "5/5 [==============================] - 0s 778us/step - loss: 117.6000\n",
            "Epoch 681/2000\n",
            "5/5 [==============================] - 0s 599us/step - loss: 117.6000\n",
            "Epoch 682/2000\n",
            "5/5 [==============================] - 0s 514us/step - loss: 117.6000\n",
            "Epoch 683/2000\n",
            "5/5 [==============================] - 0s 490us/step - loss: 117.6000\n",
            "Epoch 684/2000\n",
            "5/5 [==============================] - 0s 493us/step - loss: 117.6000\n",
            "Epoch 685/2000\n",
            "5/5 [==============================] - 0s 939us/step - loss: 117.6000\n",
            "Epoch 686/2000\n",
            "5/5 [==============================] - 0s 718us/step - loss: 117.6000\n",
            "Epoch 687/2000\n",
            "5/5 [==============================] - 0s 920us/step - loss: 117.6000\n",
            "Epoch 688/2000\n",
            "5/5 [==============================] - 0s 941us/step - loss: 117.6000\n",
            "Epoch 689/2000\n",
            "5/5 [==============================] - 0s 774us/step - loss: 117.6000\n",
            "Epoch 690/2000\n",
            "5/5 [==============================] - 0s 660us/step - loss: 117.6000\n",
            "Epoch 691/2000\n",
            "5/5 [==============================] - 0s 774us/step - loss: 117.6000\n",
            "Epoch 692/2000\n",
            "5/5 [==============================] - 0s 882us/step - loss: 117.6000\n",
            "Epoch 693/2000\n",
            "5/5 [==============================] - 0s 672us/step - loss: 117.6000\n",
            "Epoch 694/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 695/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 696/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 697/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 698/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 699/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 700/2000\n",
            "5/5 [==============================] - 0s 937us/step - loss: 117.6000\n",
            "Epoch 701/2000\n",
            "5/5 [==============================] - 0s 928us/step - loss: 117.6000\n",
            "Epoch 702/2000\n",
            "5/5 [==============================] - 0s 587us/step - loss: 117.6000\n",
            "Epoch 703/2000\n",
            "5/5 [==============================] - 0s 724us/step - loss: 117.6000\n",
            "Epoch 704/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 705/2000\n",
            "5/5 [==============================] - 0s 605us/step - loss: 117.6000\n",
            "Epoch 706/2000\n",
            "5/5 [==============================] - 0s 752us/step - loss: 117.6000\n",
            "Epoch 707/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 708/2000\n",
            "5/5 [==============================] - 0s 853us/step - loss: 117.6000\n",
            "Epoch 709/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 710/2000\n",
            "5/5 [==============================] - 0s 862us/step - loss: 117.6000\n",
            "Epoch 711/2000\n",
            "5/5 [==============================] - 0s 959us/step - loss: 117.6000\n",
            "Epoch 712/2000\n",
            "5/5 [==============================] - 0s 705us/step - loss: 117.6000\n",
            "Epoch 713/2000\n",
            "5/5 [==============================] - 0s 769us/step - loss: 117.6000\n",
            "Epoch 714/2000\n",
            "5/5 [==============================] - 0s 686us/step - loss: 117.6000\n",
            "Epoch 715/2000\n",
            "5/5 [==============================] - 0s 609us/step - loss: 117.6000\n",
            "Epoch 716/2000\n",
            "5/5 [==============================] - 0s 490us/step - loss: 117.6000\n",
            "Epoch 717/2000\n",
            "5/5 [==============================] - 0s 770us/step - loss: 117.6000\n",
            "Epoch 718/2000\n",
            "5/5 [==============================] - 0s 744us/step - loss: 117.6000\n",
            "Epoch 719/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 720/2000\n",
            "5/5 [==============================] - 0s 715us/step - loss: 117.6000\n",
            "Epoch 721/2000\n",
            "5/5 [==============================] - 0s 974us/step - loss: 117.6000\n",
            "Epoch 722/2000\n",
            "5/5 [==============================] - 0s 699us/step - loss: 117.6000\n",
            "Epoch 723/2000\n",
            "5/5 [==============================] - 0s 748us/step - loss: 117.6000\n",
            "Epoch 724/2000\n",
            "5/5 [==============================] - 0s 835us/step - loss: 117.6000\n",
            "Epoch 725/2000\n",
            "5/5 [==============================] - 0s 724us/step - loss: 117.6000\n",
            "Epoch 726/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 727/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 728/2000\n",
            "5/5 [==============================] - 0s 933us/step - loss: 117.6000\n",
            "Epoch 729/2000\n",
            "5/5 [==============================] - 0s 538us/step - loss: 117.6000\n",
            "Epoch 730/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 731/2000\n",
            "5/5 [==============================] - 0s 851us/step - loss: 117.6000\n",
            "Epoch 732/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 733/2000\n",
            "5/5 [==============================] - 0s 734us/step - loss: 117.6000\n",
            "Epoch 734/2000\n",
            "5/5 [==============================] - 0s 928us/step - loss: 117.6000\n",
            "Epoch 735/2000\n",
            "5/5 [==============================] - 0s 770us/step - loss: 117.6000\n",
            "Epoch 736/2000\n",
            "5/5 [==============================] - 0s 444us/step - loss: 117.6000\n",
            "Epoch 737/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 738/2000\n",
            "5/5 [==============================] - 0s 635us/step - loss: 117.6000\n",
            "Epoch 739/2000\n",
            "5/5 [==============================] - 0s 987us/step - loss: 117.6000\n",
            "Epoch 740/2000\n",
            "5/5 [==============================] - 0s 783us/step - loss: 117.6000\n",
            "Epoch 741/2000\n",
            "5/5 [==============================] - 0s 446us/step - loss: 117.6000\n",
            "Epoch 742/2000\n",
            "5/5 [==============================] - 0s 663us/step - loss: 117.6000\n",
            "Epoch 743/2000\n",
            "5/5 [==============================] - 0s 712us/step - loss: 117.6000\n",
            "Epoch 744/2000\n",
            "5/5 [==============================] - 0s 574us/step - loss: 117.6000\n",
            "Epoch 745/2000\n",
            "5/5 [==============================] - 0s 757us/step - loss: 117.6000\n",
            "Epoch 746/2000\n",
            "5/5 [==============================] - 0s 878us/step - loss: 117.6000\n",
            "Epoch 747/2000\n",
            "5/5 [==============================] - 0s 834us/step - loss: 117.6000\n",
            "Epoch 748/2000\n",
            "5/5 [==============================] - 0s 790us/step - loss: 117.6000\n",
            "Epoch 749/2000\n",
            "5/5 [==============================] - 0s 792us/step - loss: 117.6000\n",
            "Epoch 750/2000\n",
            "5/5 [==============================] - 0s 840us/step - loss: 117.6000\n",
            "Epoch 751/2000\n",
            "5/5 [==============================] - 0s 943us/step - loss: 117.6000\n",
            "Epoch 752/2000\n",
            "5/5 [==============================] - 0s 774us/step - loss: 117.6000\n",
            "Epoch 753/2000\n",
            "5/5 [==============================] - 0s 822us/step - loss: 117.6000\n",
            "Epoch 754/2000\n",
            "5/5 [==============================] - 0s 546us/step - loss: 117.6000\n",
            "Epoch 755/2000\n",
            "5/5 [==============================] - 0s 576us/step - loss: 117.6000\n",
            "Epoch 756/2000\n",
            "5/5 [==============================] - 0s 498us/step - loss: 117.6000\n",
            "Epoch 757/2000\n",
            "5/5 [==============================] - 0s 623us/step - loss: 117.6000\n",
            "Epoch 758/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 759/2000\n",
            "5/5 [==============================] - 0s 586us/step - loss: 117.6000\n",
            "Epoch 760/2000\n",
            "5/5 [==============================] - 0s 559us/step - loss: 117.6000\n",
            "Epoch 761/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 762/2000\n",
            "5/5 [==============================] - 0s 534us/step - loss: 117.6000\n",
            "Epoch 763/2000\n",
            "5/5 [==============================] - 0s 632us/step - loss: 117.6000\n",
            "Epoch 764/2000\n",
            "5/5 [==============================] - 0s 596us/step - loss: 117.6000\n",
            "Epoch 765/2000\n",
            "5/5 [==============================] - 0s 624us/step - loss: 117.6000\n",
            "Epoch 766/2000\n",
            "5/5 [==============================] - 0s 549us/step - loss: 117.6000\n",
            "Epoch 767/2000\n",
            "5/5 [==============================] - 0s 567us/step - loss: 117.6000\n",
            "Epoch 768/2000\n",
            "5/5 [==============================] - 0s 699us/step - loss: 117.6000\n",
            "Epoch 769/2000\n",
            "5/5 [==============================] - 0s 844us/step - loss: 117.6000\n",
            "Epoch 770/2000\n",
            "5/5 [==============================] - 0s 479us/step - loss: 117.6000\n",
            "Epoch 771/2000\n",
            "5/5 [==============================] - 0s 476us/step - loss: 117.6000\n",
            "Epoch 772/2000\n",
            "5/5 [==============================] - 0s 653us/step - loss: 117.6000\n",
            "Epoch 773/2000\n",
            "5/5 [==============================] - 0s 755us/step - loss: 117.6000\n",
            "Epoch 774/2000\n",
            "5/5 [==============================] - 0s 780us/step - loss: 117.6000\n",
            "Epoch 775/2000\n",
            "5/5 [==============================] - 0s 551us/step - loss: 117.6000\n",
            "Epoch 776/2000\n",
            "5/5 [==============================] - 0s 702us/step - loss: 117.6000\n",
            "Epoch 777/2000\n",
            "5/5 [==============================] - 0s 548us/step - loss: 117.6000\n",
            "Epoch 778/2000\n",
            "5/5 [==============================] - 0s 558us/step - loss: 117.6000\n",
            "Epoch 779/2000\n",
            "5/5 [==============================] - 0s 541us/step - loss: 117.6000\n",
            "Epoch 780/2000\n",
            "5/5 [==============================] - 0s 778us/step - loss: 117.6000\n",
            "Epoch 781/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 782/2000\n",
            "5/5 [==============================] - 0s 793us/step - loss: 117.6000\n",
            "Epoch 783/2000\n",
            "5/5 [==============================] - 0s 474us/step - loss: 117.6000\n",
            "Epoch 784/2000\n",
            "5/5 [==============================] - 0s 573us/step - loss: 117.6000\n",
            "Epoch 785/2000\n",
            "5/5 [==============================] - 0s 556us/step - loss: 117.6000\n",
            "Epoch 786/2000\n",
            "5/5 [==============================] - 0s 560us/step - loss: 117.6000\n",
            "Epoch 787/2000\n",
            "5/5 [==============================] - 0s 612us/step - loss: 117.6000\n",
            "Epoch 788/2000\n",
            "5/5 [==============================] - 0s 443us/step - loss: 117.6000\n",
            "Epoch 789/2000\n",
            "5/5 [==============================] - 0s 514us/step - loss: 117.6000\n",
            "Epoch 790/2000\n",
            "5/5 [==============================] - 0s 610us/step - loss: 117.6000\n",
            "Epoch 791/2000\n",
            "5/5 [==============================] - 0s 617us/step - loss: 117.6000\n",
            "Epoch 792/2000\n",
            "5/5 [==============================] - 0s 705us/step - loss: 117.6000\n",
            "Epoch 793/2000\n",
            "5/5 [==============================] - 0s 628us/step - loss: 117.6000\n",
            "Epoch 794/2000\n",
            "5/5 [==============================] - 0s 708us/step - loss: 117.6000\n",
            "Epoch 795/2000\n",
            "5/5 [==============================] - 0s 854us/step - loss: 117.6000\n",
            "Epoch 796/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 797/2000\n",
            "5/5 [==============================] - 0s 609us/step - loss: 117.6000\n",
            "Epoch 798/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 799/2000\n",
            "5/5 [==============================] - 0s 553us/step - loss: 117.6000\n",
            "Epoch 800/2000\n",
            "5/5 [==============================] - 0s 830us/step - loss: 117.6000\n",
            "Epoch 801/2000\n",
            "5/5 [==============================] - 0s 520us/step - loss: 117.6000\n",
            "Epoch 802/2000\n",
            "5/5 [==============================] - 0s 515us/step - loss: 117.6000\n",
            "Epoch 803/2000\n",
            "5/5 [==============================] - 0s 625us/step - loss: 117.6000\n",
            "Epoch 804/2000\n",
            "5/5 [==============================] - 0s 520us/step - loss: 117.6000\n",
            "Epoch 805/2000\n",
            "5/5 [==============================] - 0s 570us/step - loss: 117.6000\n",
            "Epoch 806/2000\n",
            "5/5 [==============================] - 0s 724us/step - loss: 117.6000\n",
            "Epoch 807/2000\n",
            "5/5 [==============================] - 0s 539us/step - loss: 117.6000\n",
            "Epoch 808/2000\n",
            "5/5 [==============================] - 0s 649us/step - loss: 117.6000\n",
            "Epoch 809/2000\n",
            "5/5 [==============================] - 0s 562us/step - loss: 117.6000\n",
            "Epoch 810/2000\n",
            "5/5 [==============================] - 0s 624us/step - loss: 117.6000\n",
            "Epoch 811/2000\n",
            "5/5 [==============================] - 0s 517us/step - loss: 117.6000\n",
            "Epoch 812/2000\n",
            "5/5 [==============================] - 0s 536us/step - loss: 117.6000\n",
            "Epoch 813/2000\n",
            "5/5 [==============================] - 0s 574us/step - loss: 117.6000\n",
            "Epoch 814/2000\n",
            "5/5 [==============================] - 0s 587us/step - loss: 117.6000\n",
            "Epoch 815/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 816/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 817/2000\n",
            "5/5 [==============================] - 0s 902us/step - loss: 117.6000\n",
            "Epoch 818/2000\n",
            "5/5 [==============================] - 0s 670us/step - loss: 117.6000\n",
            "Epoch 819/2000\n",
            "5/5 [==============================] - 0s 642us/step - loss: 117.6000\n",
            "Epoch 820/2000\n",
            "5/5 [==============================] - 0s 452us/step - loss: 117.6000\n",
            "Epoch 821/2000\n",
            "5/5 [==============================] - 0s 879us/step - loss: 117.6000\n",
            "Epoch 822/2000\n",
            "5/5 [==============================] - 0s 819us/step - loss: 117.6000\n",
            "Epoch 823/2000\n",
            "5/5 [==============================] - 0s 939us/step - loss: 117.6000\n",
            "Epoch 824/2000\n",
            "5/5 [==============================] - 0s 934us/step - loss: 117.6000\n",
            "Epoch 825/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 826/2000\n",
            "5/5 [==============================] - 0s 925us/step - loss: 117.6000\n",
            "Epoch 827/2000\n",
            "5/5 [==============================] - 0s 952us/step - loss: 117.6000\n",
            "Epoch 828/2000\n",
            "5/5 [==============================] - 0s 952us/step - loss: 117.6000\n",
            "Epoch 829/2000\n",
            "5/5 [==============================] - 0s 977us/step - loss: 117.6000\n",
            "Epoch 830/2000\n",
            "5/5 [==============================] - 0s 889us/step - loss: 117.6000\n",
            "Epoch 831/2000\n",
            "5/5 [==============================] - 0s 960us/step - loss: 117.6000\n",
            "Epoch 832/2000\n",
            "5/5 [==============================] - 0s 546us/step - loss: 117.6000\n",
            "Epoch 833/2000\n",
            "5/5 [==============================] - 0s 501us/step - loss: 117.6000\n",
            "Epoch 834/2000\n",
            "5/5 [==============================] - 0s 844us/step - loss: 117.6000\n",
            "Epoch 835/2000\n",
            "5/5 [==============================] - 0s 768us/step - loss: 117.6000\n",
            "Epoch 836/2000\n",
            "5/5 [==============================] - 0s 735us/step - loss: 117.6000\n",
            "Epoch 837/2000\n",
            "5/5 [==============================] - 0s 812us/step - loss: 117.6000\n",
            "Epoch 838/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 839/2000\n",
            "5/5 [==============================] - 0s 898us/step - loss: 117.6000\n",
            "Epoch 840/2000\n",
            "5/5 [==============================] - 0s 944us/step - loss: 117.6000\n",
            "Epoch 841/2000\n",
            "5/5 [==============================] - 0s 912us/step - loss: 117.6000\n",
            "Epoch 842/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 843/2000\n",
            "5/5 [==============================] - 0s 901us/step - loss: 117.6000\n",
            "Epoch 844/2000\n",
            "5/5 [==============================] - 0s 957us/step - loss: 117.6000\n",
            "Epoch 845/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 846/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 847/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 848/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 849/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 850/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 851/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 852/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 853/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 854/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 855/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 856/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 857/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 858/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 859/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 860/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 861/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 862/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 863/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 864/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 865/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 866/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 867/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 868/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 869/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 870/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 871/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 872/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 873/2000\n",
            "5/5 [==============================] - 0s 791us/step - loss: 117.6000\n",
            "Epoch 874/2000\n",
            "5/5 [==============================] - 0s 990us/step - loss: 117.6000\n",
            "Epoch 875/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 876/2000\n",
            "5/5 [==============================] - 0s 630us/step - loss: 117.6000\n",
            "Epoch 877/2000\n",
            "5/5 [==============================] - 0s 477us/step - loss: 117.6000\n",
            "Epoch 878/2000\n",
            "5/5 [==============================] - 0s 562us/step - loss: 117.6000\n",
            "Epoch 879/2000\n",
            "5/5 [==============================] - 0s 604us/step - loss: 117.6000\n",
            "Epoch 880/2000\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 117.6000\n",
            "Epoch 881/2000\n",
            "5/5 [==============================] - 0s 802us/step - loss: 117.6000\n",
            "Epoch 882/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 883/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 884/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 885/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 886/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 887/2000\n",
            "5/5 [==============================] - 0s 603us/step - loss: 117.6000\n",
            "Epoch 888/2000\n",
            "5/5 [==============================] - 0s 505us/step - loss: 117.6000\n",
            "Epoch 889/2000\n",
            "5/5 [==============================] - 0s 872us/step - loss: 117.6000\n",
            "Epoch 890/2000\n",
            "5/5 [==============================] - 0s 731us/step - loss: 117.6000\n",
            "Epoch 891/2000\n",
            "5/5 [==============================] - 0s 933us/step - loss: 117.6000\n",
            "Epoch 892/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 893/2000\n",
            "5/5 [==============================] - 0s 942us/step - loss: 117.6000\n",
            "Epoch 894/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 895/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 896/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 897/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 898/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 899/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 900/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 901/2000\n",
            "5/5 [==============================] - 0s 988us/step - loss: 117.6000\n",
            "Epoch 902/2000\n",
            "5/5 [==============================] - 0s 963us/step - loss: 117.6000\n",
            "Epoch 903/2000\n",
            "5/5 [==============================] - 0s 970us/step - loss: 117.6000\n",
            "Epoch 904/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 905/2000\n",
            "5/5 [==============================] - 0s 864us/step - loss: 117.6000\n",
            "Epoch 906/2000\n",
            "5/5 [==============================] - 0s 923us/step - loss: 117.6000\n",
            "Epoch 907/2000\n",
            "5/5 [==============================] - 0s 912us/step - loss: 117.6000\n",
            "Epoch 908/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 909/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 910/2000\n",
            "5/5 [==============================] - 0s 922us/step - loss: 117.6000\n",
            "Epoch 911/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 912/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 913/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 914/2000\n",
            "5/5 [==============================] - 0s 898us/step - loss: 117.6000\n",
            "Epoch 915/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 916/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 917/2000\n",
            "5/5 [==============================] - 0s 931us/step - loss: 117.6000\n",
            "Epoch 918/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 919/2000\n",
            "5/5 [==============================] - 0s 963us/step - loss: 117.6000\n",
            "Epoch 920/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 921/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 922/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 923/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 924/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 925/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 926/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 927/2000\n",
            "5/5 [==============================] - 0s 868us/step - loss: 117.6000\n",
            "Epoch 928/2000\n",
            "5/5 [==============================] - 0s 856us/step - loss: 117.6000\n",
            "Epoch 929/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 930/2000\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 117.6000\n",
            "Epoch 931/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 932/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 933/2000\n",
            "5/5 [==============================] - 0s 694us/step - loss: 117.6000\n",
            "Epoch 934/2000\n",
            "5/5 [==============================] - 0s 585us/step - loss: 117.6000\n",
            "Epoch 935/2000\n",
            "5/5 [==============================] - 0s 733us/step - loss: 117.6000\n",
            "Epoch 936/2000\n",
            "5/5 [==============================] - 0s 581us/step - loss: 117.6000\n",
            "Epoch 937/2000\n",
            "5/5 [==============================] - 0s 796us/step - loss: 117.6000\n",
            "Epoch 938/2000\n",
            "5/5 [==============================] - 0s 811us/step - loss: 117.6000\n",
            "Epoch 939/2000\n",
            "5/5 [==============================] - 0s 834us/step - loss: 117.6000\n",
            "Epoch 940/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 941/2000\n",
            "5/5 [==============================] - 0s 887us/step - loss: 117.6000\n",
            "Epoch 942/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 943/2000\n",
            "5/5 [==============================] - 0s 607us/step - loss: 117.6000\n",
            "Epoch 944/2000\n",
            "5/5 [==============================] - 0s 626us/step - loss: 117.6000\n",
            "Epoch 945/2000\n",
            "5/5 [==============================] - 0s 665us/step - loss: 117.6000\n",
            "Epoch 946/2000\n",
            "5/5 [==============================] - 0s 699us/step - loss: 117.6000\n",
            "Epoch 947/2000\n",
            "5/5 [==============================] - 0s 794us/step - loss: 117.6000\n",
            "Epoch 948/2000\n",
            "5/5 [==============================] - 0s 587us/step - loss: 117.6000\n",
            "Epoch 949/2000\n",
            "5/5 [==============================] - 0s 682us/step - loss: 117.6000\n",
            "Epoch 950/2000\n",
            "5/5 [==============================] - 0s 903us/step - loss: 117.6000\n",
            "Epoch 951/2000\n",
            "5/5 [==============================] - 0s 666us/step - loss: 117.6000\n",
            "Epoch 952/2000\n",
            "5/5 [==============================] - 0s 760us/step - loss: 117.6000\n",
            "Epoch 953/2000\n",
            "5/5 [==============================] - 0s 742us/step - loss: 117.6000\n",
            "Epoch 954/2000\n",
            "5/5 [==============================] - 0s 711us/step - loss: 117.6000\n",
            "Epoch 955/2000\n",
            "5/5 [==============================] - 0s 686us/step - loss: 117.6000\n",
            "Epoch 956/2000\n",
            "5/5 [==============================] - 0s 974us/step - loss: 117.6000\n",
            "Epoch 957/2000\n",
            "5/5 [==============================] - 0s 651us/step - loss: 117.6000\n",
            "Epoch 958/2000\n",
            "5/5 [==============================] - 0s 440us/step - loss: 117.6000\n",
            "Epoch 959/2000\n",
            "5/5 [==============================] - 0s 470us/step - loss: 117.6000\n",
            "Epoch 960/2000\n",
            "5/5 [==============================] - 0s 532us/step - loss: 117.6000\n",
            "Epoch 961/2000\n",
            "5/5 [==============================] - 0s 618us/step - loss: 117.6000\n",
            "Epoch 962/2000\n",
            "5/5 [==============================] - 0s 539us/step - loss: 117.6000\n",
            "Epoch 963/2000\n",
            "5/5 [==============================] - 0s 573us/step - loss: 117.6000\n",
            "Epoch 964/2000\n",
            "5/5 [==============================] - 0s 580us/step - loss: 117.6000\n",
            "Epoch 965/2000\n",
            "5/5 [==============================] - 0s 535us/step - loss: 117.6000\n",
            "Epoch 966/2000\n",
            "5/5 [==============================] - 0s 871us/step - loss: 117.6000\n",
            "Epoch 967/2000\n",
            "5/5 [==============================] - 0s 528us/step - loss: 117.6000\n",
            "Epoch 968/2000\n",
            "5/5 [==============================] - 0s 500us/step - loss: 117.6000\n",
            "Epoch 969/2000\n",
            "5/5 [==============================] - 0s 968us/step - loss: 117.6000\n",
            "Epoch 970/2000\n",
            "5/5 [==============================] - 0s 796us/step - loss: 117.6000\n",
            "Epoch 971/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 972/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 973/2000\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 117.6000\n",
            "Epoch 974/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 975/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 976/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 977/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 978/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 979/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 980/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 981/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 982/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 983/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 984/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 985/2000\n",
            "5/5 [==============================] - 0s 979us/step - loss: 117.6000\n",
            "Epoch 986/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 987/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 988/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 989/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 990/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 991/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 992/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 993/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 994/2000\n",
            "5/5 [==============================] - 0s 977us/step - loss: 117.6000\n",
            "Epoch 995/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 996/2000\n",
            "5/5 [==============================] - 0s 927us/step - loss: 117.6000\n",
            "Epoch 997/2000\n",
            "5/5 [==============================] - 0s 926us/step - loss: 117.6000\n",
            "Epoch 998/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 999/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1000/2000\n",
            "5/5 [==============================] - 0s 972us/step - loss: 117.6000\n",
            "Epoch 1001/2000\n",
            "5/5 [==============================] - 0s 706us/step - loss: 117.6000\n",
            "Epoch 1002/2000\n",
            "5/5 [==============================] - 0s 984us/step - loss: 117.6000\n",
            "Epoch 1003/2000\n",
            "5/5 [==============================] - 0s 869us/step - loss: 117.6000\n",
            "Epoch 1004/2000\n",
            "5/5 [==============================] - 0s 699us/step - loss: 117.6000\n",
            "Epoch 1005/2000\n",
            "5/5 [==============================] - 0s 750us/step - loss: 117.6000\n",
            "Epoch 1006/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1007/2000\n",
            "5/5 [==============================] - 0s 741us/step - loss: 117.6000\n",
            "Epoch 1008/2000\n",
            "5/5 [==============================] - 0s 933us/step - loss: 117.6000\n",
            "Epoch 1009/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1010/2000\n",
            "5/5 [==============================] - 0s 902us/step - loss: 117.6000\n",
            "Epoch 1011/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1012/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1013/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1014/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1015/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1016/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1017/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1018/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1019/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1020/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1021/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1022/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1023/2000\n",
            "5/5 [==============================] - 0s 921us/step - loss: 117.6000\n",
            "Epoch 1024/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1025/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1026/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1027/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1028/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1029/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1030/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1031/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1032/2000\n",
            "5/5 [==============================] - 0s 994us/step - loss: 117.6000\n",
            "Epoch 1033/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1034/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1035/2000\n",
            "5/5 [==============================] - 0s 994us/step - loss: 117.6000\n",
            "Epoch 1036/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1037/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1038/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1039/2000\n",
            "5/5 [==============================] - 0s 949us/step - loss: 117.6000\n",
            "Epoch 1040/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1041/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1042/2000\n",
            "5/5 [==============================] - 0s 988us/step - loss: 117.6000\n",
            "Epoch 1043/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1044/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1045/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1046/2000\n",
            "5/5 [==============================] - 0s 994us/step - loss: 117.6000\n",
            "Epoch 1047/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1048/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1049/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1050/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1051/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1052/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1053/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1054/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1055/2000\n",
            "5/5 [==============================] - 0s 925us/step - loss: 117.6000\n",
            "Epoch 1056/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1057/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1058/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1059/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1060/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1061/2000\n",
            "5/5 [==============================] - 0s 974us/step - loss: 117.6000\n",
            "Epoch 1062/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1063/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1064/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1065/2000\n",
            "5/5 [==============================] - 0s 670us/step - loss: 117.6000\n",
            "Epoch 1066/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1067/2000\n",
            "5/5 [==============================] - 0s 573us/step - loss: 117.6000\n",
            "Epoch 1068/2000\n",
            "5/5 [==============================] - 0s 678us/step - loss: 117.6000\n",
            "Epoch 1069/2000\n",
            "5/5 [==============================] - 0s 843us/step - loss: 117.6000\n",
            "Epoch 1070/2000\n",
            "5/5 [==============================] - 0s 591us/step - loss: 117.6000\n",
            "Epoch 1071/2000\n",
            "5/5 [==============================] - 0s 754us/step - loss: 117.6000\n",
            "Epoch 1072/2000\n",
            "5/5 [==============================] - 0s 632us/step - loss: 117.6000\n",
            "Epoch 1073/2000\n",
            "5/5 [==============================] - 0s 676us/step - loss: 117.6000\n",
            "Epoch 1074/2000\n",
            "5/5 [==============================] - 0s 898us/step - loss: 117.6000\n",
            "Epoch 1075/2000\n",
            "5/5 [==============================] - 0s 471us/step - loss: 117.6000\n",
            "Epoch 1076/2000\n",
            "5/5 [==============================] - 0s 748us/step - loss: 117.6000\n",
            "Epoch 1077/2000\n",
            "5/5 [==============================] - 0s 800us/step - loss: 117.6000\n",
            "Epoch 1078/2000\n",
            "5/5 [==============================] - 0s 688us/step - loss: 117.6000\n",
            "Epoch 1079/2000\n",
            "5/5 [==============================] - 0s 742us/step - loss: 117.6000\n",
            "Epoch 1080/2000\n",
            "5/5 [==============================] - 0s 454us/step - loss: 117.6000\n",
            "Epoch 1081/2000\n",
            "5/5 [==============================] - 0s 517us/step - loss: 117.6000\n",
            "Epoch 1082/2000\n",
            "5/5 [==============================] - 0s 491us/step - loss: 117.6000\n",
            "Epoch 1083/2000\n",
            "5/5 [==============================] - 0s 512us/step - loss: 117.6000\n",
            "Epoch 1084/2000\n",
            "5/5 [==============================] - 0s 503us/step - loss: 117.6000\n",
            "Epoch 1085/2000\n",
            "5/5 [==============================] - 0s 458us/step - loss: 117.6000\n",
            "Epoch 1086/2000\n",
            "5/5 [==============================] - 0s 529us/step - loss: 117.6000\n",
            "Epoch 1087/2000\n",
            "5/5 [==============================] - 0s 611us/step - loss: 117.6000\n",
            "Epoch 1088/2000\n",
            "5/5 [==============================] - 0s 511us/step - loss: 117.6000\n",
            "Epoch 1089/2000\n",
            "5/5 [==============================] - 0s 487us/step - loss: 117.6000\n",
            "Epoch 1090/2000\n",
            "5/5 [==============================] - 0s 539us/step - loss: 117.6000\n",
            "Epoch 1091/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1092/2000\n",
            "5/5 [==============================] - 0s 719us/step - loss: 117.6000\n",
            "Epoch 1093/2000\n",
            "5/5 [==============================] - 0s 707us/step - loss: 117.6000\n",
            "Epoch 1094/2000\n",
            "5/5 [==============================] - 0s 634us/step - loss: 117.6000\n",
            "Epoch 1095/2000\n",
            "5/5 [==============================] - 0s 595us/step - loss: 117.6000\n",
            "Epoch 1096/2000\n",
            "5/5 [==============================] - 0s 496us/step - loss: 117.6000\n",
            "Epoch 1097/2000\n",
            "5/5 [==============================] - 0s 501us/step - loss: 117.6000\n",
            "Epoch 1098/2000\n",
            "5/5 [==============================] - 0s 508us/step - loss: 117.6000\n",
            "Epoch 1099/2000\n",
            "5/5 [==============================] - 0s 565us/step - loss: 117.6000\n",
            "Epoch 1100/2000\n",
            "5/5 [==============================] - 0s 472us/step - loss: 117.6000\n",
            "Epoch 1101/2000\n",
            "5/5 [==============================] - 0s 426us/step - loss: 117.6000\n",
            "Epoch 1102/2000\n",
            "5/5 [==============================] - 0s 617us/step - loss: 117.6000\n",
            "Epoch 1103/2000\n",
            "5/5 [==============================] - 0s 670us/step - loss: 117.6000\n",
            "Epoch 1104/2000\n",
            "5/5 [==============================] - 0s 851us/step - loss: 117.6000\n",
            "Epoch 1105/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1106/2000\n",
            "5/5 [==============================] - 0s 910us/step - loss: 117.6000\n",
            "Epoch 1107/2000\n",
            "5/5 [==============================] - 0s 917us/step - loss: 117.6000\n",
            "Epoch 1108/2000\n",
            "5/5 [==============================] - 0s 857us/step - loss: 117.6000\n",
            "Epoch 1109/2000\n",
            "5/5 [==============================] - 0s 972us/step - loss: 117.6000\n",
            "Epoch 1110/2000\n",
            "5/5 [==============================] - 0s 910us/step - loss: 117.6000\n",
            "Epoch 1111/2000\n",
            "5/5 [==============================] - 0s 946us/step - loss: 117.6000\n",
            "Epoch 1112/2000\n",
            "5/5 [==============================] - 0s 934us/step - loss: 117.6000\n",
            "Epoch 1113/2000\n",
            "5/5 [==============================] - 0s 962us/step - loss: 117.6000\n",
            "Epoch 1114/2000\n",
            "5/5 [==============================] - 0s 732us/step - loss: 117.6000\n",
            "Epoch 1115/2000\n",
            "5/5 [==============================] - 0s 872us/step - loss: 117.6000\n",
            "Epoch 1116/2000\n",
            "5/5 [==============================] - 0s 838us/step - loss: 117.6000\n",
            "Epoch 1117/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1118/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1119/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1120/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1121/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1122/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1123/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1124/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1125/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1126/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1127/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1128/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1129/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1130/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1131/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1132/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1133/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1134/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1135/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1136/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1137/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1138/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1139/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1140/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1141/2000\n",
            "5/5 [==============================] - 0s 954us/step - loss: 117.6000\n",
            "Epoch 1142/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1143/2000\n",
            "5/5 [==============================] - 0s 960us/step - loss: 117.6000\n",
            "Epoch 1144/2000\n",
            "5/5 [==============================] - 0s 867us/step - loss: 117.6000\n",
            "Epoch 1145/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1146/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1147/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1148/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1149/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1150/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1151/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1152/2000\n",
            "5/5 [==============================] - 0s 981us/step - loss: 117.6000\n",
            "Epoch 1153/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1154/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1155/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1156/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1157/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1158/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1159/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1160/2000\n",
            "5/5 [==============================] - 0s 960us/step - loss: 117.6000\n",
            "Epoch 1161/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1162/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1163/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1164/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1165/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1166/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1167/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1168/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1169/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1170/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1171/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1172/2000\n",
            "5/5 [==============================] - 0s 919us/step - loss: 117.6000\n",
            "Epoch 1173/2000\n",
            "5/5 [==============================] - 0s 709us/step - loss: 117.6000\n",
            "Epoch 1174/2000\n",
            "5/5 [==============================] - 0s 643us/step - loss: 117.6000\n",
            "Epoch 1175/2000\n",
            "5/5 [==============================] - 0s 803us/step - loss: 117.6000\n",
            "Epoch 1176/2000\n",
            "5/5 [==============================] - 0s 673us/step - loss: 117.6000\n",
            "Epoch 1177/2000\n",
            "5/5 [==============================] - 0s 777us/step - loss: 117.6000\n",
            "Epoch 1178/2000\n",
            "5/5 [==============================] - 0s 909us/step - loss: 117.6000\n",
            "Epoch 1179/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1180/2000\n",
            "5/5 [==============================] - 0s 602us/step - loss: 117.6000\n",
            "Epoch 1181/2000\n",
            "5/5 [==============================] - 0s 753us/step - loss: 117.6000\n",
            "Epoch 1182/2000\n",
            "5/5 [==============================] - 0s 743us/step - loss: 117.6000\n",
            "Epoch 1183/2000\n",
            "5/5 [==============================] - 0s 600us/step - loss: 117.6000\n",
            "Epoch 1184/2000\n",
            "5/5 [==============================] - 0s 632us/step - loss: 117.6000\n",
            "Epoch 1185/2000\n",
            "5/5 [==============================] - 0s 881us/step - loss: 117.6000\n",
            "Epoch 1186/2000\n",
            "5/5 [==============================] - 0s 665us/step - loss: 117.6000\n",
            "Epoch 1187/2000\n",
            "5/5 [==============================] - 0s 601us/step - loss: 117.6000\n",
            "Epoch 1188/2000\n",
            "5/5 [==============================] - 0s 582us/step - loss: 117.6000\n",
            "Epoch 1189/2000\n",
            "5/5 [==============================] - 0s 691us/step - loss: 117.6000\n",
            "Epoch 1190/2000\n",
            "5/5 [==============================] - 0s 593us/step - loss: 117.6000\n",
            "Epoch 1191/2000\n",
            "5/5 [==============================] - 0s 806us/step - loss: 117.6000\n",
            "Epoch 1192/2000\n",
            "5/5 [==============================] - 0s 800us/step - loss: 117.6000\n",
            "Epoch 1193/2000\n",
            "5/5 [==============================] - 0s 747us/step - loss: 117.6000\n",
            "Epoch 1194/2000\n",
            "5/5 [==============================] - 0s 890us/step - loss: 117.6000\n",
            "Epoch 1195/2000\n",
            "5/5 [==============================] - 0s 782us/step - loss: 117.6000\n",
            "Epoch 1196/2000\n",
            "5/5 [==============================] - 0s 927us/step - loss: 117.6000\n",
            "Epoch 1197/2000\n",
            "5/5 [==============================] - 0s 703us/step - loss: 117.6000\n",
            "Epoch 1198/2000\n",
            "5/5 [==============================] - 0s 588us/step - loss: 117.6000\n",
            "Epoch 1199/2000\n",
            "5/5 [==============================] - 0s 626us/step - loss: 117.6000\n",
            "Epoch 1200/2000\n",
            "5/5 [==============================] - 0s 672us/step - loss: 117.6000\n",
            "Epoch 1201/2000\n",
            "5/5 [==============================] - 0s 508us/step - loss: 117.6000\n",
            "Epoch 1202/2000\n",
            "5/5 [==============================] - 0s 594us/step - loss: 117.6000\n",
            "Epoch 1203/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1204/2000\n",
            "5/5 [==============================] - 0s 806us/step - loss: 117.6000\n",
            "Epoch 1205/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1206/2000\n",
            "5/5 [==============================] - 0s 774us/step - loss: 117.6000\n",
            "Epoch 1207/2000\n",
            "5/5 [==============================] - 0s 772us/step - loss: 117.6000\n",
            "Epoch 1208/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1209/2000\n",
            "5/5 [==============================] - 0s 720us/step - loss: 117.6000\n",
            "Epoch 1210/2000\n",
            "5/5 [==============================] - 0s 771us/step - loss: 117.6000\n",
            "Epoch 1211/2000\n",
            "5/5 [==============================] - 0s 970us/step - loss: 117.6000\n",
            "Epoch 1212/2000\n",
            "5/5 [==============================] - 0s 602us/step - loss: 117.6000\n",
            "Epoch 1213/2000\n",
            "5/5 [==============================] - 0s 715us/step - loss: 117.6000\n",
            "Epoch 1214/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1215/2000\n",
            "5/5 [==============================] - 0s 962us/step - loss: 117.6000\n",
            "Epoch 1216/2000\n",
            "5/5 [==============================] - 0s 956us/step - loss: 117.6000\n",
            "Epoch 1217/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1218/2000\n",
            "5/5 [==============================] - 0s 752us/step - loss: 117.6000\n",
            "Epoch 1219/2000\n",
            "5/5 [==============================] - 0s 876us/step - loss: 117.6000\n",
            "Epoch 1220/2000\n",
            "5/5 [==============================] - 0s 870us/step - loss: 117.6000\n",
            "Epoch 1221/2000\n",
            "5/5 [==============================] - 0s 963us/step - loss: 117.6000\n",
            "Epoch 1222/2000\n",
            "5/5 [==============================] - 0s 980us/step - loss: 117.6000\n",
            "Epoch 1223/2000\n",
            "5/5 [==============================] - 0s 978us/step - loss: 117.6000\n",
            "Epoch 1224/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1225/2000\n",
            "5/5 [==============================] - 0s 781us/step - loss: 117.6000\n",
            "Epoch 1226/2000\n",
            "5/5 [==============================] - 0s 711us/step - loss: 117.6000\n",
            "Epoch 1227/2000\n",
            "5/5 [==============================] - 0s 919us/step - loss: 117.6000\n",
            "Epoch 1228/2000\n",
            "5/5 [==============================] - 0s 905us/step - loss: 117.6000\n",
            "Epoch 1229/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1230/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1231/2000\n",
            "5/5 [==============================] - 0s 828us/step - loss: 117.6000\n",
            "Epoch 1232/2000\n",
            "5/5 [==============================] - 0s 656us/step - loss: 117.6000\n",
            "Epoch 1233/2000\n",
            "5/5 [==============================] - 0s 617us/step - loss: 117.6000\n",
            "Epoch 1234/2000\n",
            "5/5 [==============================] - 0s 727us/step - loss: 117.6000\n",
            "Epoch 1235/2000\n",
            "5/5 [==============================] - 0s 746us/step - loss: 117.6000\n",
            "Epoch 1236/2000\n",
            "5/5 [==============================] - 0s 686us/step - loss: 117.6000\n",
            "Epoch 1237/2000\n",
            "5/5 [==============================] - 0s 618us/step - loss: 117.6000\n",
            "Epoch 1238/2000\n",
            "5/5 [==============================] - 0s 688us/step - loss: 117.6000\n",
            "Epoch 1239/2000\n",
            "5/5 [==============================] - 0s 595us/step - loss: 117.6000\n",
            "Epoch 1240/2000\n",
            "5/5 [==============================] - 0s 597us/step - loss: 117.6000\n",
            "Epoch 1241/2000\n",
            "5/5 [==============================] - 0s 637us/step - loss: 117.6000\n",
            "Epoch 1242/2000\n",
            "5/5 [==============================] - 0s 819us/step - loss: 117.6000\n",
            "Epoch 1243/2000\n",
            "5/5 [==============================] - 0s 748us/step - loss: 117.6000\n",
            "Epoch 1244/2000\n",
            "5/5 [==============================] - 0s 776us/step - loss: 117.6000\n",
            "Epoch 1245/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1246/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1247/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1248/2000\n",
            "5/5 [==============================] - 0s 791us/step - loss: 117.6000\n",
            "Epoch 1249/2000\n",
            "5/5 [==============================] - 0s 676us/step - loss: 117.6000\n",
            "Epoch 1250/2000\n",
            "5/5 [==============================] - 0s 774us/step - loss: 117.6000\n",
            "Epoch 1251/2000\n",
            "5/5 [==============================] - 0s 721us/step - loss: 117.6000\n",
            "Epoch 1252/2000\n",
            "5/5 [==============================] - 0s 879us/step - loss: 117.6000\n",
            "Epoch 1253/2000\n",
            "5/5 [==============================] - 0s 958us/step - loss: 117.6000\n",
            "Epoch 1254/2000\n",
            "5/5 [==============================] - 0s 814us/step - loss: 117.6000\n",
            "Epoch 1255/2000\n",
            "5/5 [==============================] - 0s 944us/step - loss: 117.6000\n",
            "Epoch 1256/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1257/2000\n",
            "5/5 [==============================] - 0s 749us/step - loss: 117.6000\n",
            "Epoch 1258/2000\n",
            "5/5 [==============================] - 0s 833us/step - loss: 117.6000\n",
            "Epoch 1259/2000\n",
            "5/5 [==============================] - 0s 982us/step - loss: 117.6000\n",
            "Epoch 1260/2000\n",
            "5/5 [==============================] - 0s 867us/step - loss: 117.6000\n",
            "Epoch 1261/2000\n",
            "5/5 [==============================] - 0s 804us/step - loss: 117.6000\n",
            "Epoch 1262/2000\n",
            "5/5 [==============================] - 0s 838us/step - loss: 117.6000\n",
            "Epoch 1263/2000\n",
            "5/5 [==============================] - 0s 694us/step - loss: 117.6000\n",
            "Epoch 1264/2000\n",
            "5/5 [==============================] - 0s 788us/step - loss: 117.6000\n",
            "Epoch 1265/2000\n",
            "5/5 [==============================] - 0s 663us/step - loss: 117.6000\n",
            "Epoch 1266/2000\n",
            "5/5 [==============================] - 0s 846us/step - loss: 117.6000\n",
            "Epoch 1267/2000\n",
            "5/5 [==============================] - 0s 947us/step - loss: 117.6000\n",
            "Epoch 1268/2000\n",
            "5/5 [==============================] - 0s 897us/step - loss: 117.6000\n",
            "Epoch 1269/2000\n",
            "5/5 [==============================] - 0s 833us/step - loss: 117.6000\n",
            "Epoch 1270/2000\n",
            "5/5 [==============================] - 0s 784us/step - loss: 117.6000\n",
            "Epoch 1271/2000\n",
            "5/5 [==============================] - 0s 708us/step - loss: 117.6000\n",
            "Epoch 1272/2000\n",
            "5/5 [==============================] - 0s 670us/step - loss: 117.6000\n",
            "Epoch 1273/2000\n",
            "5/5 [==============================] - 0s 909us/step - loss: 117.6000\n",
            "Epoch 1274/2000\n",
            "5/5 [==============================] - 0s 489us/step - loss: 117.6000\n",
            "Epoch 1275/2000\n",
            "5/5 [==============================] - 0s 736us/step - loss: 117.6000\n",
            "Epoch 1276/2000\n",
            "5/5 [==============================] - 0s 677us/step - loss: 117.6000\n",
            "Epoch 1277/2000\n",
            "5/5 [==============================] - 0s 666us/step - loss: 117.6000\n",
            "Epoch 1278/2000\n",
            "5/5 [==============================] - 0s 487us/step - loss: 117.6000\n",
            "Epoch 1279/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 1280/2000\n",
            "5/5 [==============================] - 0s 939us/step - loss: 117.6000\n",
            "Epoch 1281/2000\n",
            "5/5 [==============================] - 0s 529us/step - loss: 117.6000\n",
            "Epoch 1282/2000\n",
            "5/5 [==============================] - 0s 531us/step - loss: 117.6000\n",
            "Epoch 1283/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1284/2000\n",
            "5/5 [==============================] - 0s 503us/step - loss: 117.6000\n",
            "Epoch 1285/2000\n",
            "5/5 [==============================] - 0s 575us/step - loss: 117.6000\n",
            "Epoch 1286/2000\n",
            "5/5 [==============================] - 0s 502us/step - loss: 117.6000\n",
            "Epoch 1287/2000\n",
            "5/5 [==============================] - 0s 552us/step - loss: 117.6000\n",
            "Epoch 1288/2000\n",
            "5/5 [==============================] - 0s 633us/step - loss: 117.6000\n",
            "Epoch 1289/2000\n",
            "5/5 [==============================] - 0s 655us/step - loss: 117.6000\n",
            "Epoch 1290/2000\n",
            "5/5 [==============================] - 0s 569us/step - loss: 117.6000\n",
            "Epoch 1291/2000\n",
            "5/5 [==============================] - 0s 552us/step - loss: 117.6000\n",
            "Epoch 1292/2000\n",
            "5/5 [==============================] - 0s 621us/step - loss: 117.6000\n",
            "Epoch 1293/2000\n",
            "5/5 [==============================] - 0s 692us/step - loss: 117.6000\n",
            "Epoch 1294/2000\n",
            "5/5 [==============================] - 0s 837us/step - loss: 117.6000\n",
            "Epoch 1295/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 1296/2000\n",
            "5/5 [==============================] - 0s 721us/step - loss: 117.6000\n",
            "Epoch 1297/2000\n",
            "5/5 [==============================] - 0s 708us/step - loss: 117.6000\n",
            "Epoch 1298/2000\n",
            "5/5 [==============================] - 0s 547us/step - loss: 117.6000\n",
            "Epoch 1299/2000\n",
            "5/5 [==============================] - 0s 597us/step - loss: 117.6000\n",
            "Epoch 1300/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1301/2000\n",
            "5/5 [==============================] - 0s 729us/step - loss: 117.6000\n",
            "Epoch 1302/2000\n",
            "5/5 [==============================] - 0s 712us/step - loss: 117.6000\n",
            "Epoch 1303/2000\n",
            "5/5 [==============================] - 0s 667us/step - loss: 117.6000\n",
            "Epoch 1304/2000\n",
            "5/5 [==============================] - 0s 656us/step - loss: 117.6000\n",
            "Epoch 1305/2000\n",
            "5/5 [==============================] - 0s 639us/step - loss: 117.6000\n",
            "Epoch 1306/2000\n",
            "5/5 [==============================] - 0s 597us/step - loss: 117.6000\n",
            "Epoch 1307/2000\n",
            "5/5 [==============================] - 0s 660us/step - loss: 117.6000\n",
            "Epoch 1308/2000\n",
            "5/5 [==============================] - 0s 603us/step - loss: 117.6000\n",
            "Epoch 1309/2000\n",
            "5/5 [==============================] - 0s 596us/step - loss: 117.6000\n",
            "Epoch 1310/2000\n",
            "5/5 [==============================] - 0s 604us/step - loss: 117.6000\n",
            "Epoch 1311/2000\n",
            "5/5 [==============================] - 0s 625us/step - loss: 117.6000\n",
            "Epoch 1312/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1313/2000\n",
            "5/5 [==============================] - 0s 574us/step - loss: 117.6000\n",
            "Epoch 1314/2000\n",
            "5/5 [==============================] - 0s 591us/step - loss: 117.6000\n",
            "Epoch 1315/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1316/2000\n",
            "5/5 [==============================] - 0s 696us/step - loss: 117.6000\n",
            "Epoch 1317/2000\n",
            "5/5 [==============================] - 0s 644us/step - loss: 117.6000\n",
            "Epoch 1318/2000\n",
            "5/5 [==============================] - 0s 644us/step - loss: 117.6000\n",
            "Epoch 1319/2000\n",
            "5/5 [==============================] - 0s 628us/step - loss: 117.6000\n",
            "Epoch 1320/2000\n",
            "5/5 [==============================] - 0s 531us/step - loss: 117.6000\n",
            "Epoch 1321/2000\n",
            "5/5 [==============================] - 0s 557us/step - loss: 117.6000\n",
            "Epoch 1322/2000\n",
            "5/5 [==============================] - 0s 504us/step - loss: 117.6000\n",
            "Epoch 1323/2000\n",
            "5/5 [==============================] - 0s 618us/step - loss: 117.6000\n",
            "Epoch 1324/2000\n",
            "5/5 [==============================] - 0s 543us/step - loss: 117.6000\n",
            "Epoch 1325/2000\n",
            "5/5 [==============================] - 0s 558us/step - loss: 117.6000\n",
            "Epoch 1326/2000\n",
            "5/5 [==============================] - 0s 530us/step - loss: 117.6000\n",
            "Epoch 1327/2000\n",
            "5/5 [==============================] - 0s 549us/step - loss: 117.6000\n",
            "Epoch 1328/2000\n",
            "5/5 [==============================] - 0s 593us/step - loss: 117.6000\n",
            "Epoch 1329/2000\n",
            "5/5 [==============================] - 0s 577us/step - loss: 117.6000\n",
            "Epoch 1330/2000\n",
            "5/5 [==============================] - 0s 581us/step - loss: 117.6000\n",
            "Epoch 1331/2000\n",
            "5/5 [==============================] - 0s 536us/step - loss: 117.6000\n",
            "Epoch 1332/2000\n",
            "5/5 [==============================] - 0s 605us/step - loss: 117.6000\n",
            "Epoch 1333/2000\n",
            "5/5 [==============================] - 0s 597us/step - loss: 117.6000\n",
            "Epoch 1334/2000\n",
            "5/5 [==============================] - 0s 528us/step - loss: 117.6000\n",
            "Epoch 1335/2000\n",
            "5/5 [==============================] - 0s 562us/step - loss: 117.6000\n",
            "Epoch 1336/2000\n",
            "5/5 [==============================] - 0s 548us/step - loss: 117.6000\n",
            "Epoch 1337/2000\n",
            "5/5 [==============================] - 0s 585us/step - loss: 117.6000\n",
            "Epoch 1338/2000\n",
            "5/5 [==============================] - 0s 543us/step - loss: 117.6000\n",
            "Epoch 1339/2000\n",
            "5/5 [==============================] - 0s 554us/step - loss: 117.6000\n",
            "Epoch 1340/2000\n",
            "5/5 [==============================] - 0s 571us/step - loss: 117.6000\n",
            "Epoch 1341/2000\n",
            "5/5 [==============================] - 0s 577us/step - loss: 117.6000\n",
            "Epoch 1342/2000\n",
            "5/5 [==============================] - 0s 612us/step - loss: 117.6000\n",
            "Epoch 1343/2000\n",
            "5/5 [==============================] - 0s 509us/step - loss: 117.6000\n",
            "Epoch 1344/2000\n",
            "5/5 [==============================] - 0s 502us/step - loss: 117.6000\n",
            "Epoch 1345/2000\n",
            "5/5 [==============================] - 0s 533us/step - loss: 117.6000\n",
            "Epoch 1346/2000\n",
            "5/5 [==============================] - 0s 527us/step - loss: 117.6000\n",
            "Epoch 1347/2000\n",
            "5/5 [==============================] - 0s 602us/step - loss: 117.6000\n",
            "Epoch 1348/2000\n",
            "5/5 [==============================] - 0s 714us/step - loss: 117.6000\n",
            "Epoch 1349/2000\n",
            "5/5 [==============================] - 0s 836us/step - loss: 117.6000\n",
            "Epoch 1350/2000\n",
            "5/5 [==============================] - 0s 658us/step - loss: 117.6000\n",
            "Epoch 1351/2000\n",
            "5/5 [==============================] - 0s 862us/step - loss: 117.6000\n",
            "Epoch 1352/2000\n",
            "5/5 [==============================] - 0s 478us/step - loss: 117.6000\n",
            "Epoch 1353/2000\n",
            "5/5 [==============================] - 0s 550us/step - loss: 117.6000\n",
            "Epoch 1354/2000\n",
            "5/5 [==============================] - 0s 508us/step - loss: 117.6000\n",
            "Epoch 1355/2000\n",
            "5/5 [==============================] - 0s 637us/step - loss: 117.6000\n",
            "Epoch 1356/2000\n",
            "5/5 [==============================] - 0s 652us/step - loss: 117.6000\n",
            "Epoch 1357/2000\n",
            "5/5 [==============================] - 0s 511us/step - loss: 117.6000\n",
            "Epoch 1358/2000\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 117.6000\n",
            "Epoch 1359/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1360/2000\n",
            "5/5 [==============================] - 0s 679us/step - loss: 117.6000\n",
            "Epoch 1361/2000\n",
            "5/5 [==============================] - 0s 653us/step - loss: 117.6000\n",
            "Epoch 1362/2000\n",
            "5/5 [==============================] - 0s 640us/step - loss: 117.6000\n",
            "Epoch 1363/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 1364/2000\n",
            "5/5 [==============================] - 0s 629us/step - loss: 117.6000\n",
            "Epoch 1365/2000\n",
            "5/5 [==============================] - 0s 763us/step - loss: 117.6000\n",
            "Epoch 1366/2000\n",
            "5/5 [==============================] - 0s 754us/step - loss: 117.6000\n",
            "Epoch 1367/2000\n",
            "5/5 [==============================] - 0s 749us/step - loss: 117.6000\n",
            "Epoch 1368/2000\n",
            "5/5 [==============================] - 0s 775us/step - loss: 117.6000\n",
            "Epoch 1369/2000\n",
            "5/5 [==============================] - 0s 729us/step - loss: 117.6000\n",
            "Epoch 1370/2000\n",
            "5/5 [==============================] - 0s 753us/step - loss: 117.6000\n",
            "Epoch 1371/2000\n",
            "5/5 [==============================] - 0s 800us/step - loss: 117.6000\n",
            "Epoch 1372/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1373/2000\n",
            "5/5 [==============================] - 0s 843us/step - loss: 117.6000\n",
            "Epoch 1374/2000\n",
            "5/5 [==============================] - 0s 672us/step - loss: 117.6000\n",
            "Epoch 1375/2000\n",
            "5/5 [==============================] - 0s 721us/step - loss: 117.6000\n",
            "Epoch 1376/2000\n",
            "5/5 [==============================] - 0s 773us/step - loss: 117.6000\n",
            "Epoch 1377/2000\n",
            "5/5 [==============================] - 0s 718us/step - loss: 117.6000\n",
            "Epoch 1378/2000\n",
            "5/5 [==============================] - 0s 858us/step - loss: 117.6000\n",
            "Epoch 1379/2000\n",
            "5/5 [==============================] - 0s 943us/step - loss: 117.6000\n",
            "Epoch 1380/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1381/2000\n",
            "5/5 [==============================] - 0s 913us/step - loss: 117.6000\n",
            "Epoch 1382/2000\n",
            "5/5 [==============================] - 0s 833us/step - loss: 117.6000\n",
            "Epoch 1383/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1384/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1385/2000\n",
            "5/5 [==============================] - 0s 652us/step - loss: 117.6000\n",
            "Epoch 1386/2000\n",
            "5/5 [==============================] - 0s 681us/step - loss: 117.6000\n",
            "Epoch 1387/2000\n",
            "5/5 [==============================] - 0s 641us/step - loss: 117.6000\n",
            "Epoch 1388/2000\n",
            "5/5 [==============================] - 0s 622us/step - loss: 117.6000\n",
            "Epoch 1389/2000\n",
            "5/5 [==============================] - 0s 627us/step - loss: 117.6000\n",
            "Epoch 1390/2000\n",
            "5/5 [==============================] - 0s 628us/step - loss: 117.6000\n",
            "Epoch 1391/2000\n",
            "5/5 [==============================] - 0s 631us/step - loss: 117.6000\n",
            "Epoch 1392/2000\n",
            "5/5 [==============================] - 0s 971us/step - loss: 117.6000\n",
            "Epoch 1393/2000\n",
            "5/5 [==============================] - 0s 720us/step - loss: 117.6000\n",
            "Epoch 1394/2000\n",
            "5/5 [==============================] - 0s 638us/step - loss: 117.6000\n",
            "Epoch 1395/2000\n",
            "5/5 [==============================] - 0s 594us/step - loss: 117.6000\n",
            "Epoch 1396/2000\n",
            "5/5 [==============================] - 0s 604us/step - loss: 117.6000\n",
            "Epoch 1397/2000\n",
            "5/5 [==============================] - 0s 589us/step - loss: 117.6000\n",
            "Epoch 1398/2000\n",
            "5/5 [==============================] - 0s 619us/step - loss: 117.6000\n",
            "Epoch 1399/2000\n",
            "5/5 [==============================] - 0s 635us/step - loss: 117.6000\n",
            "Epoch 1400/2000\n",
            "5/5 [==============================] - 0s 780us/step - loss: 117.6000\n",
            "Epoch 1401/2000\n",
            "5/5 [==============================] - 0s 772us/step - loss: 117.6000\n",
            "Epoch 1402/2000\n",
            "5/5 [==============================] - 0s 655us/step - loss: 117.6000\n",
            "Epoch 1403/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1404/2000\n",
            "5/5 [==============================] - 0s 819us/step - loss: 117.6000\n",
            "Epoch 1405/2000\n",
            "5/5 [==============================] - 0s 985us/step - loss: 117.6000\n",
            "Epoch 1406/2000\n",
            "5/5 [==============================] - 0s 879us/step - loss: 117.6000\n",
            "Epoch 1407/2000\n",
            "5/5 [==============================] - 0s 816us/step - loss: 117.6000\n",
            "Epoch 1408/2000\n",
            "5/5 [==============================] - 0s 880us/step - loss: 117.6000\n",
            "Epoch 1409/2000\n",
            "5/5 [==============================] - 0s 869us/step - loss: 117.6000\n",
            "Epoch 1410/2000\n",
            "5/5 [==============================] - 0s 968us/step - loss: 117.6000\n",
            "Epoch 1411/2000\n",
            "5/5 [==============================] - 0s 818us/step - loss: 117.6000\n",
            "Epoch 1412/2000\n",
            "5/5 [==============================] - 0s 833us/step - loss: 117.6000\n",
            "Epoch 1413/2000\n",
            "5/5 [==============================] - 0s 856us/step - loss: 117.6000\n",
            "Epoch 1414/2000\n",
            "5/5 [==============================] - 0s 941us/step - loss: 117.6000\n",
            "Epoch 1415/2000\n",
            "5/5 [==============================] - 0s 927us/step - loss: 117.6000\n",
            "Epoch 1416/2000\n",
            "5/5 [==============================] - 0s 856us/step - loss: 117.6000\n",
            "Epoch 1417/2000\n",
            "5/5 [==============================] - 0s 927us/step - loss: 117.6000\n",
            "Epoch 1418/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1419/2000\n",
            "5/5 [==============================] - 0s 877us/step - loss: 117.6000\n",
            "Epoch 1420/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1421/2000\n",
            "5/5 [==============================] - 0s 595us/step - loss: 117.6000\n",
            "Epoch 1422/2000\n",
            "5/5 [==============================] - 0s 544us/step - loss: 117.6000\n",
            "Epoch 1423/2000\n",
            "5/5 [==============================] - 0s 611us/step - loss: 117.6000\n",
            "Epoch 1424/2000\n",
            "5/5 [==============================] - 0s 629us/step - loss: 117.6000\n",
            "Epoch 1425/2000\n",
            "5/5 [==============================] - 0s 626us/step - loss: 117.6000\n",
            "Epoch 1426/2000\n",
            "5/5 [==============================] - 0s 698us/step - loss: 117.6000\n",
            "Epoch 1427/2000\n",
            "5/5 [==============================] - 0s 503us/step - loss: 117.6000\n",
            "Epoch 1428/2000\n",
            "5/5 [==============================] - 0s 469us/step - loss: 117.6000\n",
            "Epoch 1429/2000\n",
            "5/5 [==============================] - 0s 548us/step - loss: 117.6000\n",
            "Epoch 1430/2000\n",
            "5/5 [==============================] - 0s 465us/step - loss: 117.6000\n",
            "Epoch 1431/2000\n",
            "5/5 [==============================] - 0s 615us/step - loss: 117.6000\n",
            "Epoch 1432/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1433/2000\n",
            "5/5 [==============================] - 0s 754us/step - loss: 117.6000\n",
            "Epoch 1434/2000\n",
            "5/5 [==============================] - 0s 542us/step - loss: 117.6000\n",
            "Epoch 1435/2000\n",
            "5/5 [==============================] - 0s 817us/step - loss: 117.6000\n",
            "Epoch 1436/2000\n",
            "5/5 [==============================] - 0s 792us/step - loss: 117.6000\n",
            "Epoch 1437/2000\n",
            "5/5 [==============================] - 0s 586us/step - loss: 117.6000\n",
            "Epoch 1438/2000\n",
            "5/5 [==============================] - 0s 536us/step - loss: 117.6000\n",
            "Epoch 1439/2000\n",
            "5/5 [==============================] - 0s 734us/step - loss: 117.6000\n",
            "Epoch 1440/2000\n",
            "5/5 [==============================] - 0s 846us/step - loss: 117.6000\n",
            "Epoch 1441/2000\n",
            "5/5 [==============================] - 0s 907us/step - loss: 117.6000\n",
            "Epoch 1442/2000\n",
            "5/5 [==============================] - 0s 585us/step - loss: 117.6000\n",
            "Epoch 1443/2000\n",
            "5/5 [==============================] - 0s 733us/step - loss: 117.6000\n",
            "Epoch 1444/2000\n",
            "5/5 [==============================] - 0s 575us/step - loss: 117.6000\n",
            "Epoch 1445/2000\n",
            "5/5 [==============================] - 0s 898us/step - loss: 117.6000\n",
            "Epoch 1446/2000\n",
            "5/5 [==============================] - 0s 600us/step - loss: 117.6000\n",
            "Epoch 1447/2000\n",
            "5/5 [==============================] - 0s 702us/step - loss: 117.6000\n",
            "Epoch 1448/2000\n",
            "5/5 [==============================] - 0s 555us/step - loss: 117.6000\n",
            "Epoch 1449/2000\n",
            "5/5 [==============================] - 0s 565us/step - loss: 117.6000\n",
            "Epoch 1450/2000\n",
            "5/5 [==============================] - 0s 600us/step - loss: 117.6000\n",
            "Epoch 1451/2000\n",
            "5/5 [==============================] - 0s 587us/step - loss: 117.6000\n",
            "Epoch 1452/2000\n",
            "5/5 [==============================] - 0s 640us/step - loss: 117.6000\n",
            "Epoch 1453/2000\n",
            "5/5 [==============================] - 0s 755us/step - loss: 117.6000\n",
            "Epoch 1454/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1455/2000\n",
            "5/5 [==============================] - 0s 738us/step - loss: 117.6000\n",
            "Epoch 1456/2000\n",
            "5/5 [==============================] - 0s 857us/step - loss: 117.6000\n",
            "Epoch 1457/2000\n",
            "5/5 [==============================] - 0s 668us/step - loss: 117.6000\n",
            "Epoch 1458/2000\n",
            "5/5 [==============================] - 0s 781us/step - loss: 117.6000\n",
            "Epoch 1459/2000\n",
            "5/5 [==============================] - 0s 494us/step - loss: 117.6000\n",
            "Epoch 1460/2000\n",
            "5/5 [==============================] - 0s 437us/step - loss: 117.6000\n",
            "Epoch 1461/2000\n",
            "5/5 [==============================] - 0s 757us/step - loss: 117.6000\n",
            "Epoch 1462/2000\n",
            "5/5 [==============================] - 0s 546us/step - loss: 117.6000\n",
            "Epoch 1463/2000\n",
            "5/5 [==============================] - 0s 723us/step - loss: 117.6000\n",
            "Epoch 1464/2000\n",
            "5/5 [==============================] - 0s 523us/step - loss: 117.6000\n",
            "Epoch 1465/2000\n",
            "5/5 [==============================] - 0s 598us/step - loss: 117.6000\n",
            "Epoch 1466/2000\n",
            "5/5 [==============================] - 0s 593us/step - loss: 117.6000\n",
            "Epoch 1467/2000\n",
            "5/5 [==============================] - 0s 609us/step - loss: 117.6000\n",
            "Epoch 1468/2000\n",
            "5/5 [==============================] - 0s 543us/step - loss: 117.6000\n",
            "Epoch 1469/2000\n",
            "5/5 [==============================] - 0s 559us/step - loss: 117.6000\n",
            "Epoch 1470/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1471/2000\n",
            "5/5 [==============================] - 0s 663us/step - loss: 117.6000\n",
            "Epoch 1472/2000\n",
            "5/5 [==============================] - 0s 552us/step - loss: 117.6000\n",
            "Epoch 1473/2000\n",
            "5/5 [==============================] - 0s 548us/step - loss: 117.6000\n",
            "Epoch 1474/2000\n",
            "5/5 [==============================] - 0s 544us/step - loss: 117.6000\n",
            "Epoch 1475/2000\n",
            "5/5 [==============================] - 0s 620us/step - loss: 117.6000\n",
            "Epoch 1476/2000\n",
            "5/5 [==============================] - 0s 519us/step - loss: 117.6000\n",
            "Epoch 1477/2000\n",
            "5/5 [==============================] - 0s 509us/step - loss: 117.6000\n",
            "Epoch 1478/2000\n",
            "5/5 [==============================] - 0s 523us/step - loss: 117.6000\n",
            "Epoch 1479/2000\n",
            "5/5 [==============================] - 0s 556us/step - loss: 117.6000\n",
            "Epoch 1480/2000\n",
            "5/5 [==============================] - 0s 681us/step - loss: 117.6000\n",
            "Epoch 1481/2000\n",
            "5/5 [==============================] - 0s 603us/step - loss: 117.6000\n",
            "Epoch 1482/2000\n",
            "5/5 [==============================] - 0s 521us/step - loss: 117.6000\n",
            "Epoch 1483/2000\n",
            "5/5 [==============================] - 0s 541us/step - loss: 117.6000\n",
            "Epoch 1484/2000\n",
            "5/5 [==============================] - 0s 626us/step - loss: 117.6000\n",
            "Epoch 1485/2000\n",
            "5/5 [==============================] - 0s 715us/step - loss: 117.6000\n",
            "Epoch 1486/2000\n",
            "5/5 [==============================] - 0s 539us/step - loss: 117.6000\n",
            "Epoch 1487/2000\n",
            "5/5 [==============================] - 0s 569us/step - loss: 117.6000\n",
            "Epoch 1488/2000\n",
            "5/5 [==============================] - 0s 502us/step - loss: 117.6000\n",
            "Epoch 1489/2000\n",
            "5/5 [==============================] - 0s 578us/step - loss: 117.6000\n",
            "Epoch 1490/2000\n",
            "5/5 [==============================] - 0s 564us/step - loss: 117.6000\n",
            "Epoch 1491/2000\n",
            "5/5 [==============================] - 0s 510us/step - loss: 117.6000\n",
            "Epoch 1492/2000\n",
            "5/5 [==============================] - 0s 488us/step - loss: 117.6000\n",
            "Epoch 1493/2000\n",
            "5/5 [==============================] - 0s 550us/step - loss: 117.6000\n",
            "Epoch 1494/2000\n",
            "5/5 [==============================] - 0s 521us/step - loss: 117.6000\n",
            "Epoch 1495/2000\n",
            "5/5 [==============================] - 0s 708us/step - loss: 117.6000\n",
            "Epoch 1496/2000\n",
            "5/5 [==============================] - 0s 480us/step - loss: 117.6000\n",
            "Epoch 1497/2000\n",
            "5/5 [==============================] - 0s 725us/step - loss: 117.6000\n",
            "Epoch 1498/2000\n",
            "5/5 [==============================] - 0s 772us/step - loss: 117.6000\n",
            "Epoch 1499/2000\n",
            "5/5 [==============================] - 0s 669us/step - loss: 117.6000\n",
            "Epoch 1500/2000\n",
            "5/5 [==============================] - 0s 560us/step - loss: 117.6000\n",
            "Epoch 1501/2000\n",
            "5/5 [==============================] - 0s 551us/step - loss: 117.6000\n",
            "Epoch 1502/2000\n",
            "5/5 [==============================] - 0s 535us/step - loss: 117.6000\n",
            "Epoch 1503/2000\n",
            "5/5 [==============================] - 0s 626us/step - loss: 117.6000\n",
            "Epoch 1504/2000\n",
            "5/5 [==============================] - 0s 543us/step - loss: 117.6000\n",
            "Epoch 1505/2000\n",
            "5/5 [==============================] - 0s 650us/step - loss: 117.6000\n",
            "Epoch 1506/2000\n",
            "5/5 [==============================] - 0s 551us/step - loss: 117.6000\n",
            "Epoch 1507/2000\n",
            "5/5 [==============================] - 0s 564us/step - loss: 117.6000\n",
            "Epoch 1508/2000\n",
            "5/5 [==============================] - 0s 986us/step - loss: 117.6000\n",
            "Epoch 1509/2000\n",
            "5/5 [==============================] - 0s 684us/step - loss: 117.6000\n",
            "Epoch 1510/2000\n",
            "5/5 [==============================] - 0s 672us/step - loss: 117.6000\n",
            "Epoch 1511/2000\n",
            "5/5 [==============================] - 0s 555us/step - loss: 117.6000\n",
            "Epoch 1512/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 1513/2000\n",
            "5/5 [==============================] - 0s 631us/step - loss: 117.6000\n",
            "Epoch 1514/2000\n",
            "5/5 [==============================] - 0s 552us/step - loss: 117.6000\n",
            "Epoch 1515/2000\n",
            "5/5 [==============================] - 0s 512us/step - loss: 117.6000\n",
            "Epoch 1516/2000\n",
            "5/5 [==============================] - 0s 712us/step - loss: 117.6000\n",
            "Epoch 1517/2000\n",
            "5/5 [==============================] - 0s 562us/step - loss: 117.6000\n",
            "Epoch 1518/2000\n",
            "5/5 [==============================] - 0s 549us/step - loss: 117.6000\n",
            "Epoch 1519/2000\n",
            "5/5 [==============================] - 0s 908us/step - loss: 117.6000\n",
            "Epoch 1520/2000\n",
            "5/5 [==============================] - 0s 959us/step - loss: 117.6000\n",
            "Epoch 1521/2000\n",
            "5/5 [==============================] - 0s 934us/step - loss: 117.6000\n",
            "Epoch 1522/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1523/2000\n",
            "5/5 [==============================] - 0s 980us/step - loss: 117.6000\n",
            "Epoch 1524/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1525/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1526/2000\n",
            "5/5 [==============================] - 0s 1000us/step - loss: 117.6000\n",
            "Epoch 1527/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1528/2000\n",
            "5/5 [==============================] - 0s 857us/step - loss: 117.6000\n",
            "Epoch 1529/2000\n",
            "5/5 [==============================] - 0s 991us/step - loss: 117.6000\n",
            "Epoch 1530/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1531/2000\n",
            "5/5 [==============================] - 0s 993us/step - loss: 117.6000\n",
            "Epoch 1532/2000\n",
            "5/5 [==============================] - 0s 700us/step - loss: 117.6000\n",
            "Epoch 1533/2000\n",
            "5/5 [==============================] - 0s 791us/step - loss: 117.6000\n",
            "Epoch 1534/2000\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 117.6000\n",
            "Epoch 1535/2000\n",
            "5/5 [==============================] - 0s 913us/step - loss: 117.6000\n",
            "Epoch 1536/2000\n",
            "5/5 [==============================] - 0s 739us/step - loss: 117.6000\n",
            "Epoch 1537/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1538/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1539/2000\n",
            "5/5 [==============================] - 0s 819us/step - loss: 117.6000\n",
            "Epoch 1540/2000\n",
            "5/5 [==============================] - 0s 933us/step - loss: 117.6000\n",
            "Epoch 1541/2000\n",
            "5/5 [==============================] - 0s 900us/step - loss: 117.6000\n",
            "Epoch 1542/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1543/2000\n",
            "5/5 [==============================] - 0s 781us/step - loss: 117.6000\n",
            "Epoch 1544/2000\n",
            "5/5 [==============================] - 0s 638us/step - loss: 117.6000\n",
            "Epoch 1545/2000\n",
            "5/5 [==============================] - 0s 683us/step - loss: 117.6000\n",
            "Epoch 1546/2000\n",
            "5/5 [==============================] - 0s 745us/step - loss: 117.6000\n",
            "Epoch 1547/2000\n",
            "5/5 [==============================] - 0s 618us/step - loss: 117.6000\n",
            "Epoch 1548/2000\n",
            "5/5 [==============================] - 0s 645us/step - loss: 117.6000\n",
            "Epoch 1549/2000\n",
            "5/5 [==============================] - 0s 808us/step - loss: 117.6000\n",
            "Epoch 1550/2000\n",
            "5/5 [==============================] - 0s 864us/step - loss: 117.6000\n",
            "Epoch 1551/2000\n",
            "5/5 [==============================] - 0s 605us/step - loss: 117.6000\n",
            "Epoch 1552/2000\n",
            "5/5 [==============================] - 0s 687us/step - loss: 117.6000\n",
            "Epoch 1553/2000\n",
            "5/5 [==============================] - 0s 710us/step - loss: 117.6000\n",
            "Epoch 1554/2000\n",
            "5/5 [==============================] - 0s 933us/step - loss: 117.6000\n",
            "Epoch 1555/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1556/2000\n",
            "5/5 [==============================] - 0s 785us/step - loss: 117.6000\n",
            "Epoch 1557/2000\n",
            "5/5 [==============================] - 0s 870us/step - loss: 117.6000\n",
            "Epoch 1558/2000\n",
            "5/5 [==============================] - 0s 821us/step - loss: 117.6000\n",
            "Epoch 1559/2000\n",
            "5/5 [==============================] - 0s 773us/step - loss: 117.6000\n",
            "Epoch 1560/2000\n",
            "5/5 [==============================] - 0s 817us/step - loss: 117.6000\n",
            "Epoch 1561/2000\n",
            "5/5 [==============================] - 0s 963us/step - loss: 117.6000\n",
            "Epoch 1562/2000\n",
            "5/5 [==============================] - 0s 938us/step - loss: 117.6000\n",
            "Epoch 1563/2000\n",
            "5/5 [==============================] - 0s 853us/step - loss: 117.6000\n",
            "Epoch 1564/2000\n",
            "5/5 [==============================] - 0s 930us/step - loss: 117.6000\n",
            "Epoch 1565/2000\n",
            "5/5 [==============================] - 0s 925us/step - loss: 117.6000\n",
            "Epoch 1566/2000\n",
            "5/5 [==============================] - 0s 734us/step - loss: 117.6000\n",
            "Epoch 1567/2000\n",
            "5/5 [==============================] - 0s 855us/step - loss: 117.6000\n",
            "Epoch 1568/2000\n",
            "5/5 [==============================] - 0s 901us/step - loss: 117.6000\n",
            "Epoch 1569/2000\n",
            "5/5 [==============================] - 0s 786us/step - loss: 117.6000\n",
            "Epoch 1570/2000\n",
            "5/5 [==============================] - 0s 842us/step - loss: 117.6000\n",
            "Epoch 1571/2000\n",
            "5/5 [==============================] - 0s 821us/step - loss: 117.6000\n",
            "Epoch 1572/2000\n",
            "5/5 [==============================] - 0s 837us/step - loss: 117.6000\n",
            "Epoch 1573/2000\n",
            "5/5 [==============================] - 0s 887us/step - loss: 117.6000\n",
            "Epoch 1574/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 1575/2000\n",
            "5/5 [==============================] - 0s 913us/step - loss: 117.6000\n",
            "Epoch 1576/2000\n",
            "5/5 [==============================] - 0s 835us/step - loss: 117.6000\n",
            "Epoch 1577/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1578/2000\n",
            "5/5 [==============================] - 0s 885us/step - loss: 117.6000\n",
            "Epoch 1579/2000\n",
            "5/5 [==============================] - 0s 772us/step - loss: 117.6000\n",
            "Epoch 1580/2000\n",
            "5/5 [==============================] - 0s 666us/step - loss: 117.6000\n",
            "Epoch 1581/2000\n",
            "5/5 [==============================] - 0s 548us/step - loss: 117.6000\n",
            "Epoch 1582/2000\n",
            "5/5 [==============================] - 0s 545us/step - loss: 117.6000\n",
            "Epoch 1583/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1584/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1585/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1586/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1587/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1588/2000\n",
            "5/5 [==============================] - 0s 960us/step - loss: 117.6000\n",
            "Epoch 1589/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1590/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1591/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1592/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1593/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1594/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1595/2000\n",
            "5/5 [==============================] - 0s 932us/step - loss: 117.6000\n",
            "Epoch 1596/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1597/2000\n",
            "5/5 [==============================] - 0s 947us/step - loss: 117.6000\n",
            "Epoch 1598/2000\n",
            "5/5 [==============================] - 0s 884us/step - loss: 117.6000\n",
            "Epoch 1599/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1600/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1601/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1602/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1603/2000\n",
            "5/5 [==============================] - 0s 650us/step - loss: 117.6000\n",
            "Epoch 1604/2000\n",
            "5/5 [==============================] - 0s 852us/step - loss: 117.6000\n",
            "Epoch 1605/2000\n",
            "5/5 [==============================] - 0s 794us/step - loss: 117.6000\n",
            "Epoch 1606/2000\n",
            "5/5 [==============================] - 0s 650us/step - loss: 117.6000\n",
            "Epoch 1607/2000\n",
            "5/5 [==============================] - 0s 774us/step - loss: 117.6000\n",
            "Epoch 1608/2000\n",
            "5/5 [==============================] - 0s 597us/step - loss: 117.6000\n",
            "Epoch 1609/2000\n",
            "5/5 [==============================] - 0s 694us/step - loss: 117.6000\n",
            "Epoch 1610/2000\n",
            "5/5 [==============================] - 0s 952us/step - loss: 117.6000\n",
            "Epoch 1611/2000\n",
            "5/5 [==============================] - 0s 795us/step - loss: 117.6000\n",
            "Epoch 1612/2000\n",
            "5/5 [==============================] - 0s 819us/step - loss: 117.6000\n",
            "Epoch 1613/2000\n",
            "5/5 [==============================] - 0s 733us/step - loss: 117.6000\n",
            "Epoch 1614/2000\n",
            "5/5 [==============================] - 0s 657us/step - loss: 117.6000\n",
            "Epoch 1615/2000\n",
            "5/5 [==============================] - 0s 484us/step - loss: 117.6000\n",
            "Epoch 1616/2000\n",
            "5/5 [==============================] - 0s 881us/step - loss: 117.6000\n",
            "Epoch 1617/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1618/2000\n",
            "5/5 [==============================] - 0s 744us/step - loss: 117.6000\n",
            "Epoch 1619/2000\n",
            "5/5 [==============================] - 0s 762us/step - loss: 117.6000\n",
            "Epoch 1620/2000\n",
            "5/5 [==============================] - 0s 605us/step - loss: 117.6000\n",
            "Epoch 1621/2000\n",
            "5/5 [==============================] - 0s 717us/step - loss: 117.6000\n",
            "Epoch 1622/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1623/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1624/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1625/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1626/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1627/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1628/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1629/2000\n",
            "5/5 [==============================] - 0s 674us/step - loss: 117.6000\n",
            "Epoch 1630/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1631/2000\n",
            "5/5 [==============================] - 0s 554us/step - loss: 117.6000\n",
            "Epoch 1632/2000\n",
            "5/5 [==============================] - 0s 763us/step - loss: 117.6000\n",
            "Epoch 1633/2000\n",
            "5/5 [==============================] - 0s 666us/step - loss: 117.6000\n",
            "Epoch 1634/2000\n",
            "5/5 [==============================] - 0s 925us/step - loss: 117.6000\n",
            "Epoch 1635/2000\n",
            "5/5 [==============================] - 0s 710us/step - loss: 117.6000\n",
            "Epoch 1636/2000\n",
            "5/5 [==============================] - 0s 618us/step - loss: 117.6000\n",
            "Epoch 1637/2000\n",
            "5/5 [==============================] - 0s 817us/step - loss: 117.6000\n",
            "Epoch 1638/2000\n",
            "5/5 [==============================] - 0s 690us/step - loss: 117.6000\n",
            "Epoch 1639/2000\n",
            "5/5 [==============================] - 0s 843us/step - loss: 117.6000\n",
            "Epoch 1640/2000\n",
            "5/5 [==============================] - 0s 644us/step - loss: 117.6000\n",
            "Epoch 1641/2000\n",
            "5/5 [==============================] - 0s 677us/step - loss: 117.6000\n",
            "Epoch 1642/2000\n",
            "5/5 [==============================] - 0s 685us/step - loss: 117.6000\n",
            "Epoch 1643/2000\n",
            "5/5 [==============================] - 0s 707us/step - loss: 117.6000\n",
            "Epoch 1644/2000\n",
            "5/5 [==============================] - 0s 615us/step - loss: 117.6000\n",
            "Epoch 1645/2000\n",
            "5/5 [==============================] - 0s 578us/step - loss: 117.6000\n",
            "Epoch 1646/2000\n",
            "5/5 [==============================] - 0s 588us/step - loss: 117.6000\n",
            "Epoch 1647/2000\n",
            "5/5 [==============================] - 0s 933us/step - loss: 117.6000\n",
            "Epoch 1648/2000\n",
            "5/5 [==============================] - 0s 682us/step - loss: 117.6000\n",
            "Epoch 1649/2000\n",
            "5/5 [==============================] - 0s 593us/step - loss: 117.6000\n",
            "Epoch 1650/2000\n",
            "5/5 [==============================] - 0s 770us/step - loss: 117.6000\n",
            "Epoch 1651/2000\n",
            "5/5 [==============================] - 0s 567us/step - loss: 117.6000\n",
            "Epoch 1652/2000\n",
            "5/5 [==============================] - 0s 819us/step - loss: 117.6000\n",
            "Epoch 1653/2000\n",
            "5/5 [==============================] - 0s 666us/step - loss: 117.6000\n",
            "Epoch 1654/2000\n",
            "5/5 [==============================] - 0s 652us/step - loss: 117.6000\n",
            "Epoch 1655/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1656/2000\n",
            "5/5 [==============================] - 0s 725us/step - loss: 117.6000\n",
            "Epoch 1657/2000\n",
            "5/5 [==============================] - 0s 763us/step - loss: 117.6000\n",
            "Epoch 1658/2000\n",
            "5/5 [==============================] - 0s 614us/step - loss: 117.6000\n",
            "Epoch 1659/2000\n",
            "5/5 [==============================] - 0s 619us/step - loss: 117.6000\n",
            "Epoch 1660/2000\n",
            "5/5 [==============================] - 0s 769us/step - loss: 117.6000\n",
            "Epoch 1661/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1662/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1663/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1664/2000\n",
            "5/5 [==============================] - 0s 763us/step - loss: 117.6000\n",
            "Epoch 1665/2000\n",
            "5/5 [==============================] - 0s 756us/step - loss: 117.6000\n",
            "Epoch 1666/2000\n",
            "5/5 [==============================] - 0s 665us/step - loss: 117.6000\n",
            "Epoch 1667/2000\n",
            "5/5 [==============================] - 0s 729us/step - loss: 117.6000\n",
            "Epoch 1668/2000\n",
            "5/5 [==============================] - 0s 608us/step - loss: 117.6000\n",
            "Epoch 1669/2000\n",
            "5/5 [==============================] - 0s 993us/step - loss: 117.6000\n",
            "Epoch 1670/2000\n",
            "5/5 [==============================] - 0s 839us/step - loss: 117.6000\n",
            "Epoch 1671/2000\n",
            "5/5 [==============================] - 0s 722us/step - loss: 117.6000\n",
            "Epoch 1672/2000\n",
            "5/5 [==============================] - 0s 510us/step - loss: 117.6000\n",
            "Epoch 1673/2000\n",
            "5/5 [==============================] - 0s 913us/step - loss: 117.6000\n",
            "Epoch 1674/2000\n",
            "5/5 [==============================] - 0s 813us/step - loss: 117.6000\n",
            "Epoch 1675/2000\n",
            "5/5 [==============================] - 0s 609us/step - loss: 117.6000\n",
            "Epoch 1676/2000\n",
            "5/5 [==============================] - 0s 818us/step - loss: 117.6000\n",
            "Epoch 1677/2000\n",
            "5/5 [==============================] - 0s 948us/step - loss: 117.6000\n",
            "Epoch 1678/2000\n",
            "5/5 [==============================] - 0s 557us/step - loss: 117.6000\n",
            "Epoch 1679/2000\n",
            "5/5 [==============================] - 0s 852us/step - loss: 117.6000\n",
            "Epoch 1680/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1681/2000\n",
            "5/5 [==============================] - 0s 516us/step - loss: 117.6000\n",
            "Epoch 1682/2000\n",
            "5/5 [==============================] - 0s 808us/step - loss: 117.6000\n",
            "Epoch 1683/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1684/2000\n",
            "5/5 [==============================] - 0s 981us/step - loss: 117.6000\n",
            "Epoch 1685/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1686/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1687/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1688/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1689/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1690/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1691/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1692/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1693/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1694/2000\n",
            "5/5 [==============================] - 0s 928us/step - loss: 117.6000\n",
            "Epoch 1695/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1696/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1697/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1698/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1699/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1700/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1701/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1702/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1703/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1704/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1705/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1706/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1707/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1708/2000\n",
            "5/5 [==============================] - 0s 771us/step - loss: 117.6000\n",
            "Epoch 1709/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1710/2000\n",
            "5/5 [==============================] - 0s 846us/step - loss: 117.6000\n",
            "Epoch 1711/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1712/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1713/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1714/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1715/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1716/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1717/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1718/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1719/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1720/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1721/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1722/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1723/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1724/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1725/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1726/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1727/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1728/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1729/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1730/2000\n",
            "5/5 [==============================] - 0s 838us/step - loss: 117.6000\n",
            "Epoch 1731/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1732/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1733/2000\n",
            "5/5 [==============================] - 0s 615us/step - loss: 117.6000\n",
            "Epoch 1734/2000\n",
            "5/5 [==============================] - 0s 674us/step - loss: 117.6000\n",
            "Epoch 1735/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1736/2000\n",
            "5/5 [==============================] - 0s 687us/step - loss: 117.6000\n",
            "Epoch 1737/2000\n",
            "5/5 [==============================] - 0s 841us/step - loss: 117.6000\n",
            "Epoch 1738/2000\n",
            "5/5 [==============================] - 0s 857us/step - loss: 117.6000\n",
            "Epoch 1739/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1740/2000\n",
            "5/5 [==============================] - 0s 521us/step - loss: 117.6000\n",
            "Epoch 1741/2000\n",
            "5/5 [==============================] - 0s 721us/step - loss: 117.6000\n",
            "Epoch 1742/2000\n",
            "5/5 [==============================] - 0s 613us/step - loss: 117.6000\n",
            "Epoch 1743/2000\n",
            "5/5 [==============================] - 0s 562us/step - loss: 117.6000\n",
            "Epoch 1744/2000\n",
            "5/5 [==============================] - 0s 593us/step - loss: 117.6000\n",
            "Epoch 1745/2000\n",
            "5/5 [==============================] - 0s 590us/step - loss: 117.6000\n",
            "Epoch 1746/2000\n",
            "5/5 [==============================] - 0s 705us/step - loss: 117.6000\n",
            "Epoch 1747/2000\n",
            "5/5 [==============================] - 0s 853us/step - loss: 117.6000\n",
            "Epoch 1748/2000\n",
            "5/5 [==============================] - 0s 674us/step - loss: 117.6000\n",
            "Epoch 1749/2000\n",
            "5/5 [==============================] - 0s 589us/step - loss: 117.6000\n",
            "Epoch 1750/2000\n",
            "5/5 [==============================] - 0s 711us/step - loss: 117.6000\n",
            "Epoch 1751/2000\n",
            "5/5 [==============================] - 0s 610us/step - loss: 117.6000\n",
            "Epoch 1752/2000\n",
            "5/5 [==============================] - 0s 962us/step - loss: 117.6000\n",
            "Epoch 1753/2000\n",
            "5/5 [==============================] - 0s 831us/step - loss: 117.6000\n",
            "Epoch 1754/2000\n",
            "5/5 [==============================] - 0s 820us/step - loss: 117.6000\n",
            "Epoch 1755/2000\n",
            "5/5 [==============================] - 0s 732us/step - loss: 117.6000\n",
            "Epoch 1756/2000\n",
            "5/5 [==============================] - 0s 884us/step - loss: 117.6000\n",
            "Epoch 1757/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1758/2000\n",
            "5/5 [==============================] - 0s 997us/step - loss: 117.6000\n",
            "Epoch 1759/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1760/2000\n",
            "5/5 [==============================] - 0s 963us/step - loss: 117.6000\n",
            "Epoch 1761/2000\n",
            "5/5 [==============================] - 0s 966us/step - loss: 117.6000\n",
            "Epoch 1762/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1763/2000\n",
            "5/5 [==============================] - 0s 950us/step - loss: 117.6000\n",
            "Epoch 1764/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1765/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1766/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1767/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1768/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1769/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1770/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1771/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1772/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1773/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1774/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1775/2000\n",
            "5/5 [==============================] - 0s 731us/step - loss: 117.6000\n",
            "Epoch 1776/2000\n",
            "5/5 [==============================] - 0s 879us/step - loss: 117.6000\n",
            "Epoch 1777/2000\n",
            "5/5 [==============================] - 0s 717us/step - loss: 117.6000\n",
            "Epoch 1778/2000\n",
            "5/5 [==============================] - 0s 657us/step - loss: 117.6000\n",
            "Epoch 1779/2000\n",
            "5/5 [==============================] - 0s 650us/step - loss: 117.6000\n",
            "Epoch 1780/2000\n",
            "5/5 [==============================] - 0s 610us/step - loss: 117.6000\n",
            "Epoch 1781/2000\n",
            "5/5 [==============================] - 0s 895us/step - loss: 117.6000\n",
            "Epoch 1782/2000\n",
            "5/5 [==============================] - 0s 710us/step - loss: 117.6000\n",
            "Epoch 1783/2000\n",
            "5/5 [==============================] - 0s 709us/step - loss: 117.6000\n",
            "Epoch 1784/2000\n",
            "5/5 [==============================] - 0s 816us/step - loss: 117.6000\n",
            "Epoch 1785/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1786/2000\n",
            "5/5 [==============================] - 0s 629us/step - loss: 117.6000\n",
            "Epoch 1787/2000\n",
            "5/5 [==============================] - 0s 692us/step - loss: 117.6000\n",
            "Epoch 1788/2000\n",
            "5/5 [==============================] - 0s 629us/step - loss: 117.6000\n",
            "Epoch 1789/2000\n",
            "5/5 [==============================] - 0s 673us/step - loss: 117.6000\n",
            "Epoch 1790/2000\n",
            "5/5 [==============================] - 0s 779us/step - loss: 117.6000\n",
            "Epoch 1791/2000\n",
            "5/5 [==============================] - 0s 726us/step - loss: 117.6000\n",
            "Epoch 1792/2000\n",
            "5/5 [==============================] - 0s 784us/step - loss: 117.6000\n",
            "Epoch 1793/2000\n",
            "5/5 [==============================] - 0s 757us/step - loss: 117.6000\n",
            "Epoch 1794/2000\n",
            "5/5 [==============================] - 0s 857us/step - loss: 117.6000\n",
            "Epoch 1795/2000\n",
            "5/5 [==============================] - 0s 835us/step - loss: 117.6000\n",
            "Epoch 1796/2000\n",
            "5/5 [==============================] - 0s 810us/step - loss: 117.6000\n",
            "Epoch 1797/2000\n",
            "5/5 [==============================] - 0s 823us/step - loss: 117.6000\n",
            "Epoch 1798/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1799/2000\n",
            "5/5 [==============================] - 0s 990us/step - loss: 117.6000\n",
            "Epoch 1800/2000\n",
            "5/5 [==============================] - 0s 777us/step - loss: 117.6000\n",
            "Epoch 1801/2000\n",
            "5/5 [==============================] - 0s 727us/step - loss: 117.6000\n",
            "Epoch 1802/2000\n",
            "5/5 [==============================] - 0s 746us/step - loss: 117.6000\n",
            "Epoch 1803/2000\n",
            "5/5 [==============================] - 0s 715us/step - loss: 117.6000\n",
            "Epoch 1804/2000\n",
            "5/5 [==============================] - 0s 980us/step - loss: 117.6000\n",
            "Epoch 1805/2000\n",
            "5/5 [==============================] - 0s 850us/step - loss: 117.6000\n",
            "Epoch 1806/2000\n",
            "5/5 [==============================] - 0s 833us/step - loss: 117.6000\n",
            "Epoch 1807/2000\n",
            "5/5 [==============================] - 0s 739us/step - loss: 117.6000\n",
            "Epoch 1808/2000\n",
            "5/5 [==============================] - 0s 849us/step - loss: 117.6000\n",
            "Epoch 1809/2000\n",
            "5/5 [==============================] - 0s 826us/step - loss: 117.6000\n",
            "Epoch 1810/2000\n",
            "5/5 [==============================] - 0s 814us/step - loss: 117.6000\n",
            "Epoch 1811/2000\n",
            "5/5 [==============================] - 0s 855us/step - loss: 117.6000\n",
            "Epoch 1812/2000\n",
            "5/5 [==============================] - 0s 971us/step - loss: 117.6000\n",
            "Epoch 1813/2000\n",
            "5/5 [==============================] - 0s 926us/step - loss: 117.6000\n",
            "Epoch 1814/2000\n",
            "5/5 [==============================] - 0s 738us/step - loss: 117.6000\n",
            "Epoch 1815/2000\n",
            "5/5 [==============================] - 0s 693us/step - loss: 117.6000\n",
            "Epoch 1816/2000\n",
            "5/5 [==============================] - 0s 934us/step - loss: 117.6000\n",
            "Epoch 1817/2000\n",
            "5/5 [==============================] - 0s 736us/step - loss: 117.6000\n",
            "Epoch 1818/2000\n",
            "5/5 [==============================] - 0s 908us/step - loss: 117.6000\n",
            "Epoch 1819/2000\n",
            "5/5 [==============================] - 0s 758us/step - loss: 117.6000\n",
            "Epoch 1820/2000\n",
            "5/5 [==============================] - 0s 761us/step - loss: 117.6000\n",
            "Epoch 1821/2000\n",
            "5/5 [==============================] - 0s 877us/step - loss: 117.6000\n",
            "Epoch 1822/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1823/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1824/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1825/2000\n",
            "5/5 [==============================] - 0s 865us/step - loss: 117.6000\n",
            "Epoch 1826/2000\n",
            "5/5 [==============================] - 0s 937us/step - loss: 117.6000\n",
            "Epoch 1827/2000\n",
            "5/5 [==============================] - 0s 760us/step - loss: 117.6000\n",
            "Epoch 1828/2000\n",
            "5/5 [==============================] - 0s 890us/step - loss: 117.6000\n",
            "Epoch 1829/2000\n",
            "5/5 [==============================] - 0s 646us/step - loss: 117.6000\n",
            "Epoch 1830/2000\n",
            "5/5 [==============================] - 0s 586us/step - loss: 117.6000\n",
            "Epoch 1831/2000\n",
            "5/5 [==============================] - 0s 645us/step - loss: 117.6000\n",
            "Epoch 1832/2000\n",
            "5/5 [==============================] - 0s 666us/step - loss: 117.6000\n",
            "Epoch 1833/2000\n",
            "5/5 [==============================] - 0s 566us/step - loss: 117.6000\n",
            "Epoch 1834/2000\n",
            "5/5 [==============================] - 0s 719us/step - loss: 117.6000\n",
            "Epoch 1835/2000\n",
            "5/5 [==============================] - 0s 583us/step - loss: 117.6000\n",
            "Epoch 1836/2000\n",
            "5/5 [==============================] - 0s 569us/step - loss: 117.6000\n",
            "Epoch 1837/2000\n",
            "5/5 [==============================] - 0s 712us/step - loss: 117.6000\n",
            "Epoch 1838/2000\n",
            "5/5 [==============================] - 0s 632us/step - loss: 117.6000\n",
            "Epoch 1839/2000\n",
            "5/5 [==============================] - 0s 622us/step - loss: 117.6000\n",
            "Epoch 1840/2000\n",
            "5/5 [==============================] - 0s 631us/step - loss: 117.6000\n",
            "Epoch 1841/2000\n",
            "5/5 [==============================] - 0s 645us/step - loss: 117.6000\n",
            "Epoch 1842/2000\n",
            "5/5 [==============================] - 0s 496us/step - loss: 117.6000\n",
            "Epoch 1843/2000\n",
            "5/5 [==============================] - 0s 490us/step - loss: 117.6000\n",
            "Epoch 1844/2000\n",
            "5/5 [==============================] - 0s 500us/step - loss: 117.6000\n",
            "Epoch 1845/2000\n",
            "5/5 [==============================] - 0s 501us/step - loss: 117.6000\n",
            "Epoch 1846/2000\n",
            "5/5 [==============================] - 0s 533us/step - loss: 117.6000\n",
            "Epoch 1847/2000\n",
            "5/5 [==============================] - 0s 681us/step - loss: 117.6000\n",
            "Epoch 1848/2000\n",
            "5/5 [==============================] - 0s 519us/step - loss: 117.6000\n",
            "Epoch 1849/2000\n",
            "5/5 [==============================] - 0s 626us/step - loss: 117.6000\n",
            "Epoch 1850/2000\n",
            "5/5 [==============================] - 0s 654us/step - loss: 117.6000\n",
            "Epoch 1851/2000\n",
            "5/5 [==============================] - 0s 510us/step - loss: 117.6000\n",
            "Epoch 1852/2000\n",
            "5/5 [==============================] - 0s 540us/step - loss: 117.6000\n",
            "Epoch 1853/2000\n",
            "5/5 [==============================] - 0s 698us/step - loss: 117.6000\n",
            "Epoch 1854/2000\n",
            "5/5 [==============================] - 0s 493us/step - loss: 117.6000\n",
            "Epoch 1855/2000\n",
            "5/5 [==============================] - 0s 537us/step - loss: 117.6000\n",
            "Epoch 1856/2000\n",
            "5/5 [==============================] - 0s 819us/step - loss: 117.6000\n",
            "Epoch 1857/2000\n",
            "5/5 [==============================] - 0s 512us/step - loss: 117.6000\n",
            "Epoch 1858/2000\n",
            "5/5 [==============================] - 0s 729us/step - loss: 117.6000\n",
            "Epoch 1859/2000\n",
            "5/5 [==============================] - 0s 900us/step - loss: 117.6000\n",
            "Epoch 1860/2000\n",
            "5/5 [==============================] - 0s 519us/step - loss: 117.6000\n",
            "Epoch 1861/2000\n",
            "5/5 [==============================] - 0s 499us/step - loss: 117.6000\n",
            "Epoch 1862/2000\n",
            "5/5 [==============================] - 0s 679us/step - loss: 117.6000\n",
            "Epoch 1863/2000\n",
            "5/5 [==============================] - 0s 641us/step - loss: 117.6000\n",
            "Epoch 1864/2000\n",
            "5/5 [==============================] - 0s 727us/step - loss: 117.6000\n",
            "Epoch 1865/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1866/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1867/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1868/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1869/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1870/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1871/2000\n",
            "5/5 [==============================] - 0s 923us/step - loss: 117.6000\n",
            "Epoch 1872/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1873/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1874/2000\n",
            "5/5 [==============================] - 0s 893us/step - loss: 117.6000\n",
            "Epoch 1875/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1876/2000\n",
            "5/5 [==============================] - 0s 701us/step - loss: 117.6000\n",
            "Epoch 1877/2000\n",
            "5/5 [==============================] - 0s 580us/step - loss: 117.6000\n",
            "Epoch 1878/2000\n",
            "5/5 [==============================] - 0s 725us/step - loss: 117.6000\n",
            "Epoch 1879/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1880/2000\n",
            "5/5 [==============================] - 0s 850us/step - loss: 117.6000\n",
            "Epoch 1881/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1882/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1883/2000\n",
            "5/5 [==============================] - 0s 963us/step - loss: 117.6000\n",
            "Epoch 1884/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1885/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1886/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1887/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1888/2000\n",
            "5/5 [==============================] - 0s 604us/step - loss: 117.6000\n",
            "Epoch 1889/2000\n",
            "5/5 [==============================] - 0s 576us/step - loss: 117.6000\n",
            "Epoch 1890/2000\n",
            "5/5 [==============================] - 0s 657us/step - loss: 117.6000\n",
            "Epoch 1891/2000\n",
            "5/5 [==============================] - 0s 757us/step - loss: 117.6000\n",
            "Epoch 1892/2000\n",
            "5/5 [==============================] - 0s 556us/step - loss: 117.6000\n",
            "Epoch 1893/2000\n",
            "5/5 [==============================] - 0s 717us/step - loss: 117.6000\n",
            "Epoch 1894/2000\n",
            "5/5 [==============================] - 0s 658us/step - loss: 117.6000\n",
            "Epoch 1895/2000\n",
            "5/5 [==============================] - 0s 653us/step - loss: 117.6000\n",
            "Epoch 1896/2000\n",
            "5/5 [==============================] - 0s 804us/step - loss: 117.6000\n",
            "Epoch 1897/2000\n",
            "5/5 [==============================] - 0s 636us/step - loss: 117.6000\n",
            "Epoch 1898/2000\n",
            "5/5 [==============================] - 0s 650us/step - loss: 117.6000\n",
            "Epoch 1899/2000\n",
            "5/5 [==============================] - 0s 692us/step - loss: 117.6000\n",
            "Epoch 1900/2000\n",
            "5/5 [==============================] - 0s 595us/step - loss: 117.6000\n",
            "Epoch 1901/2000\n",
            "5/5 [==============================] - 0s 667us/step - loss: 117.6000\n",
            "Epoch 1902/2000\n",
            "5/5 [==============================] - 0s 738us/step - loss: 117.6000\n",
            "Epoch 1903/2000\n",
            "5/5 [==============================] - 0s 905us/step - loss: 117.6000\n",
            "Epoch 1904/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1905/2000\n",
            "5/5 [==============================] - 0s 786us/step - loss: 117.6000\n",
            "Epoch 1906/2000\n",
            "5/5 [==============================] - 0s 688us/step - loss: 117.6000\n",
            "Epoch 1907/2000\n",
            "5/5 [==============================] - 0s 808us/step - loss: 117.6000\n",
            "Epoch 1908/2000\n",
            "5/5 [==============================] - 0s 640us/step - loss: 117.6000\n",
            "Epoch 1909/2000\n",
            "5/5 [==============================] - 0s 661us/step - loss: 117.6000\n",
            "Epoch 1910/2000\n",
            "5/5 [==============================] - 0s 639us/step - loss: 117.6000\n",
            "Epoch 1911/2000\n",
            "5/5 [==============================] - 0s 689us/step - loss: 117.6000\n",
            "Epoch 1912/2000\n",
            "5/5 [==============================] - 0s 807us/step - loss: 117.6000\n",
            "Epoch 1913/2000\n",
            "5/5 [==============================] - 0s 816us/step - loss: 117.6000\n",
            "Epoch 1914/2000\n",
            "5/5 [==============================] - 0s 788us/step - loss: 117.6000\n",
            "Epoch 1915/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1916/2000\n",
            "5/5 [==============================] - 0s 544us/step - loss: 117.6000\n",
            "Epoch 1917/2000\n",
            "5/5 [==============================] - 0s 548us/step - loss: 117.6000\n",
            "Epoch 1918/2000\n",
            "5/5 [==============================] - 0s 578us/step - loss: 117.6000\n",
            "Epoch 1919/2000\n",
            "5/5 [==============================] - 0s 522us/step - loss: 117.6000\n",
            "Epoch 1920/2000\n",
            "5/5 [==============================] - 0s 685us/step - loss: 117.6000\n",
            "Epoch 1921/2000\n",
            "5/5 [==============================] - 0s 583us/step - loss: 117.6000\n",
            "Epoch 1922/2000\n",
            "5/5 [==============================] - 0s 597us/step - loss: 117.6000\n",
            "Epoch 1923/2000\n",
            "5/5 [==============================] - 0s 605us/step - loss: 117.6000\n",
            "Epoch 1924/2000\n",
            "5/5 [==============================] - 0s 674us/step - loss: 117.6000\n",
            "Epoch 1925/2000\n",
            "5/5 [==============================] - 0s 703us/step - loss: 117.6000\n",
            "Epoch 1926/2000\n",
            "5/5 [==============================] - 0s 705us/step - loss: 117.6000\n",
            "Epoch 1927/2000\n",
            "5/5 [==============================] - 0s 590us/step - loss: 117.6000\n",
            "Epoch 1928/2000\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 117.6000\n",
            "Epoch 1929/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1930/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1931/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1932/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1933/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1934/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1935/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1936/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1937/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1938/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1939/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1940/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1941/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1942/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1943/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1944/2000\n",
            "5/5 [==============================] - 0s 891us/step - loss: 117.6000\n",
            "Epoch 1945/2000\n",
            "5/5 [==============================] - 0s 856us/step - loss: 117.6000\n",
            "Epoch 1946/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1947/2000\n",
            "5/5 [==============================] - 0s 974us/step - loss: 117.6000\n",
            "Epoch 1948/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1949/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1950/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1951/2000\n",
            "5/5 [==============================] - 0s 979us/step - loss: 117.6000\n",
            "Epoch 1952/2000\n",
            "5/5 [==============================] - 0s 891us/step - loss: 117.6000\n",
            "Epoch 1953/2000\n",
            "5/5 [==============================] - 0s 946us/step - loss: 117.6000\n",
            "Epoch 1954/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1955/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1956/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1957/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1958/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1959/2000\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 117.6000\n",
            "Epoch 1960/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1961/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1962/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1963/2000\n",
            "5/5 [==============================] - 0s 979us/step - loss: 117.6000\n",
            "Epoch 1964/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1965/2000\n",
            "5/5 [==============================] - 0s 788us/step - loss: 117.6000\n",
            "Epoch 1966/2000\n",
            "5/5 [==============================] - 0s 817us/step - loss: 117.6000\n",
            "Epoch 1967/2000\n",
            "5/5 [==============================] - 0s 611us/step - loss: 117.6000\n",
            "Epoch 1968/2000\n",
            "5/5 [==============================] - 0s 688us/step - loss: 117.6000\n",
            "Epoch 1969/2000\n",
            "5/5 [==============================] - 0s 804us/step - loss: 117.6000\n",
            "Epoch 1970/2000\n",
            "5/5 [==============================] - 0s 810us/step - loss: 117.6000\n",
            "Epoch 1971/2000\n",
            "5/5 [==============================] - 0s 699us/step - loss: 117.6000\n",
            "Epoch 1972/2000\n",
            "5/5 [==============================] - 0s 984us/step - loss: 117.6000\n",
            "Epoch 1973/2000\n",
            "5/5 [==============================] - 0s 777us/step - loss: 117.6000\n",
            "Epoch 1974/2000\n",
            "5/5 [==============================] - 0s 708us/step - loss: 117.6000\n",
            "Epoch 1975/2000\n",
            "5/5 [==============================] - 0s 506us/step - loss: 117.6000\n",
            "Epoch 1976/2000\n",
            "5/5 [==============================] - 0s 661us/step - loss: 117.6000\n",
            "Epoch 1977/2000\n",
            "5/5 [==============================] - 0s 737us/step - loss: 117.6000\n",
            "Epoch 1978/2000\n",
            "5/5 [==============================] - 0s 564us/step - loss: 117.6000\n",
            "Epoch 1979/2000\n",
            "5/5 [==============================] - 0s 611us/step - loss: 117.6000\n",
            "Epoch 1980/2000\n",
            "5/5 [==============================] - 0s 752us/step - loss: 117.6000\n",
            "Epoch 1981/2000\n",
            "5/5 [==============================] - 0s 615us/step - loss: 117.6000\n",
            "Epoch 1982/2000\n",
            "5/5 [==============================] - 0s 961us/step - loss: 117.6000\n",
            "Epoch 1983/2000\n",
            "5/5 [==============================] - 0s 650us/step - loss: 117.6000\n",
            "Epoch 1984/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n",
            "Epoch 1985/2000\n",
            "5/5 [==============================] - 0s 649us/step - loss: 117.6000\n",
            "Epoch 1986/2000\n",
            "5/5 [==============================] - 0s 799us/step - loss: 117.6000\n",
            "Epoch 1987/2000\n",
            "5/5 [==============================] - 0s 591us/step - loss: 117.6000\n",
            "Epoch 1988/2000\n",
            "5/5 [==============================] - 0s 592us/step - loss: 117.6000\n",
            "Epoch 1989/2000\n",
            "5/5 [==============================] - 0s 626us/step - loss: 117.6000\n",
            "Epoch 1990/2000\n",
            "5/5 [==============================] - 0s 545us/step - loss: 117.6000\n",
            "Epoch 1991/2000\n",
            "5/5 [==============================] - 0s 735us/step - loss: 117.6000\n",
            "Epoch 1992/2000\n",
            "5/5 [==============================] - 0s 588us/step - loss: 117.6000\n",
            "Epoch 1993/2000\n",
            "5/5 [==============================] - 0s 584us/step - loss: 117.6000\n",
            "Epoch 1994/2000\n",
            "5/5 [==============================] - 0s 668us/step - loss: 117.6000\n",
            "Epoch 1995/2000\n",
            "5/5 [==============================] - 0s 640us/step - loss: 117.6000\n",
            "Epoch 1996/2000\n",
            "5/5 [==============================] - 0s 574us/step - loss: 117.6000\n",
            "Epoch 1997/2000\n",
            "5/5 [==============================] - 0s 817us/step - loss: 117.6000\n",
            "Epoch 1998/2000\n",
            "5/5 [==============================] - 0s 619us/step - loss: 117.6000\n",
            "Epoch 1999/2000\n",
            "5/5 [==============================] - 0s 970us/step - loss: 117.6000\n",
            "Epoch 2000/2000\n",
            "5/5 [==============================] - 0s 1ms/step - loss: 117.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SoG_E_Dv1G-",
        "colab_type": "text"
      },
      "source": [
        "학습되는 과정을 loss값을 통해 확인한다.\n",
        "\n",
        "loss는 mean_squared_error, 최소제곱평균으로 정의했으므로 얼마나 y_train값에 근접하느냐를 뜻한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIr208V5v1G_",
        "colab_type": "code",
        "outputId": "297448cd-fbf1-43c2-e3d1-234308edf5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.predict(x_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_hZroev1HB",
        "colab_type": "text"
      },
      "source": [
        "[1, 1, 1], [2, 2, 2]를 test데이터로 사용하였으므로, 4와 7이 나올것으로 기대할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC3RGo_pv1HC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 15\n",
        "epochs = 300\n",
        "LearningRate = 1e-3\n",
        "Decay = 1e-6\n",
        "img_width = 224\n",
        "img_height = 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTNgwI2Qv1HE",
        "colab_type": "text"
      },
      "source": [
        "학습에 사용될 데이터의 위치를 알린다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRveTBiyyO6g",
        "colab_type": "code",
        "outputId": "514ef771-0d53-4f29-bb9a-ebae047910c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jusQx4ODySgT",
        "colab_type": "text"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO_ae99wyTJ0",
        "colab_type": "text"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe9bUfCIv1HF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CurrentDirectory = \"drive/My Drive/SNUBI/python_seminar/data (1)/\"\n",
        "\n",
        "train_directory = CurrentDirectory + 'TRAIN/'\n",
        "test_directory\t= CurrentDirectory + 'TEST/'\n",
        "model_directory = CurrentDirectory + 'MODEL/'\n",
        "tensorboard_directory = CurrentDirectory + 'Tensorboard'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sShC_YTSv1HH",
        "colab_type": "text"
      },
      "source": [
        "Convolutional Neural Network 알고리즘들 중, 대표적인 VGG16 모델의 구성요소를 직접 구현한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoEqWRdvv1HH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def VGG_16():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224,3)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\t\n",
        "\t#top layer of the VGG net\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVvA81jOv1HJ",
        "colab_type": "text"
      },
      "source": [
        "Binary classification을 구현하는 것으로, 최종 Fully connected layer의 units를 2로 설정한다.\n",
        "\n",
        "GlobalAveragePooling2D 는 최종적으로 Classification 이후, Attention Heatmap을 그리기 위한 Layer이다.\n",
        "\n",
        "softmax는 Sigmoid function의 general한 함수.\n",
        "\n",
        "momentum은 Learning rate에 의한 Gradient Descent의 방향성에 관성을 주는것. (너무 와리가리 하면 local minimum에 빠지기 쉬움)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XaTPcsRv1HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vggModel = VGG_16()\n",
        "x = GlobalAveragePooling2D()(vggModel.output)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "DeepLearning = Model(inputs=vggModel.input, outputs=predictions)\n",
        "\n",
        "DeepLearning.compile(optimizer=SGD(lr=LearningRate,decay=Decay,\n",
        "\tmomentum=0.9,nesterov=True),loss='categorical_crossentropy',metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "y0pCHkSxv1HM",
        "colab_type": "text"
      },
      "source": [
        "Keras에서 Generator는 자동으로 Data를 load해주는 역할을 함.\n",
        "\n",
        "데이터를 불러올 때, 이미지를 자동으로 전처리 하는 코드를 Argument로 삽입할 수 있음\n",
        "\n",
        "- Rescale: [0-255] 데이터를 [0-1] scale로 변환\n",
        "- rotation_range: 이미지를 각도를 돌려가면서 학습시켜줄 수 있음\n",
        "- shift_range: 이미지를 shift\n",
        "- shear_range: 이미지에 왜곡을 줌\n",
        "- zoom_rnage: 이미지를 확대\n",
        "- flip: 뒤집기\n",
        "- featurewise_center, std_normalization: 이미지 분포 변환\n",
        "- data_format: RGB 3채널은 데이터의 마지막에 들어감. ex) [224, 224, 3]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiOCZaCJv1HN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATAGEN = ImageDataGenerator(\n",
        "\trescale=1./255,\n",
        "\trotation_range=20,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.2,\n",
        "\tzoom_range=0.2, \n",
        "\thorizontal_flip=True,\n",
        "\tvertical_flip=True,\n",
        "\tfeaturewise_center=True,\n",
        "\tfeaturewise_std_normalization=True,\n",
        "\tdata_format=\"channels_last\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz4FOtu0v1HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#asd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEZqjQvjv1HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATAGEN_TEST = ImageDataGenerator(\n",
        "\trescale=1./255,\n",
        "\tfeaturewise_center=True,\n",
        "\tfeaturewise_std_normalization=True,\n",
        "\tdata_format=\"channels_last\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei3VQiQOv1HT",
        "colab_type": "text"
      },
      "source": [
        "Test 데이터 용으로 Generator를 생성.\n",
        "\n",
        "모델의 성능을 판단할 Test dataset은 Augmentation을 수행하면 안됨."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRHH9CmLv1HU",
        "colab_type": "code",
        "outputId": "966838c6-40e0-49cf-bfb3-80a2e639758e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "TRAIN_GENERATOR = DATAGEN.flow_from_directory(\n",
        "\ttrain_directory,\n",
        "\ttarget_size = (img_width, img_height),\n",
        "\tbatch_size = batch_size,\n",
        "\tclass_mode='categorical')\n",
        "\n",
        "TEST_GENERATOR = DATAGEN_TEST.flow_from_directory(\n",
        "\ttest_directory,\n",
        "\ttarget_size = (img_width, img_height),\n",
        "\tbatch_size = batch_size,\n",
        "\tshuffle = False,\n",
        "\tclass_mode='categorical')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 540 images belonging to 2 classes.\n",
            "Found 60 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrgJc3Trv1HW",
        "colab_type": "text"
      },
      "source": [
        "540개의 이미지가 Training data로 읽혀짐\n",
        "\n",
        "60개의 이미지가 Test data로 읽혀짐.\n",
        "\n",
        "단 Augmentation은 Training data에만 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyLyOchQv1HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CP = ModelCheckpoint(filepath=model_directory+\n",
        "\t\t\t\t\t'-{epoch:03d}-{val_loss:.4f}-{val_acc:.4f}.hdf5',\n",
        "\t\t\t\t\tmonitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "TB = TensorBoard(log_dir=tensorboard_directory, write_graph=True, write_images=True)\n",
        "LR = ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=3, verbose=1, min_lr=1e-8)\n",
        "CALLBACK = [CP, TB, LR]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HJKJ6Npv1HZ",
        "colab_type": "text"
      },
      "source": [
        "ModelCheckpoint는 학습을 진행하면서 생성된 모델의 weight를 저장함\n",
        "\n",
        "TensorBoard는 학습의 진행사항에 대한 log와 모델의 정보를 보는 기능\n",
        "\n",
        "LearningRate를 자동으로 수정해주는 Plateau\n",
        "\n",
        "\n",
        "위 3가지를 학습하면서 자동으로 실행하여, 매 학습마다 저장해주는 기능을 CallBack이라고 함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMcypaT3v1HZ",
        "colab_type": "code",
        "outputId": "26d9faa5-7627-4492-d145-86dac2393675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "DeepLearning.fit_generator(\n",
        "\tTRAIN_GENERATOR,\n",
        "\tsteps_per_epoch=3,\n",
        "\tepochs=200,\n",
        "\tcallbacks=CALLBACK,\n",
        "\tshuffle=True,\n",
        "\tvalidation_data=TEST_GENERATOR,\n",
        "\tvalidation_steps=1)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 15s 5s/step - loss: 0.6931 - acc: 0.5333 - val_loss: 0.6921 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to drive/My Drive/SNUBI/python_seminar/data (1)/MODEL/-001-0.6921-1.0000.hdf5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.6933 - acc: 0.3556 - val_loss: 0.6947 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 1.00000\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 12s 4s/step - loss: 0.6935 - acc: 0.4667 - val_loss: 0.6896 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 1.00000\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6881 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 1.00000\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.6937 - acc: 0.4444 - val_loss: 0.6976 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 1.00000\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 14s 5s/step - loss: 0.6934 - acc: 0.4667 - val_loss: 0.6971 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 1.00000\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 12s 4s/step - loss: 0.6935 - acc: 0.4444 - val_loss: 0.6905 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.6923 - acc: 0.6444 - val_loss: 0.6895 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 1.00000\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.6935 - acc: 0.4444 - val_loss: 0.6965 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 1.00000\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.6936 - acc: 0.4222 - val_loss: 0.6955 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 10s 3s/step - loss: 0.6932 - acc: 0.5333 - val_loss: 0.6919 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 1.00000\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.6931 - acc: 0.4667 - val_loss: 0.6926 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 1.00000\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 489ms/step - loss: 0.6933 - acc: 0.3556 - val_loss: 0.6932 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.6931 - acc: 0.6222 - val_loss: 0.6935 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 1.00000\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.6931 - acc: 0.4667 - val_loss: 0.6927 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 1.00000\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6931 - acc: 0.4667 - val_loss: 0.6922 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 2s 506ms/step - loss: 0.6931 - acc: 0.4889 - val_loss: 0.6935 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 1.00000\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6933 - val_acc: 0.0667\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 1.00000\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6931 - acc: 0.5556 - val_loss: 0.6931 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.6930 - acc: 0.6000 - val_loss: 0.6919 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 1.00000\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 494ms/step - loss: 0.6928 - acc: 0.5778 - val_loss: 0.6953 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 1.00000\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6937 - acc: 0.3333 - val_loss: 0.6949 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6912 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 1.00000\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.6934 - acc: 0.4000 - val_loss: 0.6915 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 1.00000\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 2s 529ms/step - loss: 0.6932 - acc: 0.4667 - val_loss: 0.6933 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 411ms/step - loss: 0.6931 - acc: 0.5556 - val_loss: 0.6931 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 1.00000\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 0.6929 - acc: 0.8444 - val_loss: 0.6939 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 1.00000\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 400ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6945 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 2s 516ms/step - loss: 0.6935 - acc: 0.3778 - val_loss: 0.6911 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 1.00000\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.6937 - acc: 0.2889 - val_loss: 0.6917 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 1.00000\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6938 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6929 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 1.00000\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 2s 516ms/step - loss: 0.6930 - acc: 0.7333 - val_loss: 0.6929 - val_acc: 0.8667\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 1.00000\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.6930 - acc: 0.7111 - val_loss: 0.6927 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.6930 - acc: 0.6000 - val_loss: 0.6932 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 1.00000\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6929 - acc: 0.8222 - val_loss: 0.6931 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 1.00000\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 2s 521ms/step - loss: 0.6929 - acc: 0.6444 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 1.00000\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6938 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 1.00000\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6931 - acc: 0.4444 - val_loss: 0.6935 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 2s 531ms/step - loss: 0.6927 - acc: 0.6444 - val_loss: 0.6920 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 1.00000\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 400ms/step - loss: 0.6931 - acc: 0.4444 - val_loss: 0.6919 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 1.00000\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 408ms/step - loss: 0.6929 - acc: 0.5111 - val_loss: 0.6942 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6939 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 1.00000\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 2s 551ms/step - loss: 0.6929 - acc: 0.4889 - val_loss: 0.6918 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 1.00000\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 414ms/step - loss: 0.6931 - acc: 0.4222 - val_loss: 0.6920 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6941 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 1.00000\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 408ms/step - loss: 0.6931 - acc: 0.4444 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 1.00000\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 2s 530ms/step - loss: 0.6932 - acc: 0.3778 - val_loss: 0.6922 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6923 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 1.00000\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.6930 - acc: 0.4444 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 1.00000\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 409ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6932 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 482ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6925 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 1.00000\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6929 - acc: 0.6444 - val_loss: 0.6926 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 1.00000\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6930 - acc: 0.6222 - val_loss: 0.6935 - val_acc: 0.1333\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6929 - acc: 0.6667 - val_loss: 0.6932 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 1.00000\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 493ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6925 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 1.00000\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 404ms/step - loss: 0.6930 - acc: 0.6000 - val_loss: 0.6925 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.6930 - acc: 0.6222 - val_loss: 0.6935 - val_acc: 0.0667\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 1.00000\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.6929 - acc: 0.6444 - val_loss: 0.6932 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 1.00000\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 492ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6925 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.4411519805435093e-05.\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6925 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 1.00000\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 363ms/step - loss: 0.6929 - acc: 0.6889 - val_loss: 0.6936 - val_acc: 0.0667\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 1.00000\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 362ms/step - loss: 0.6929 - acc: 0.5778 - val_loss: 0.6933 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.1529216135386379e-05.\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 2s 512ms/step - loss: 0.6931 - acc: 0.4222 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 1.00000\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6925 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 1.00000\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.6928 - acc: 0.6667 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 9.223372762789951e-06.\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6933 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 1.00000\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 495ms/step - loss: 0.6931 - acc: 0.4000 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 1.00000\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6927 - acc: 0.7333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00070: ReduceLROnPlateau reducing learning rate to 7.378698501270265e-06.\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.6930 - acc: 0.4000 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 1.00000\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.6930 - acc: 0.5778 - val_loss: 0.6933 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 1.00000\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 2s 527ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00073: ReduceLROnPlateau reducing learning rate to 5.902958946535364e-06.\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.6929 - acc: 0.5778 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 1.00000\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 1.00000\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6930 - acc: 0.5556 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 4.7223671572282915e-06.\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 2s 507ms/step - loss: 0.6929 - acc: 0.5111 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 1.00000\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 410ms/step - loss: 0.6930 - acc: 0.4222 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 1.00000\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.7778936530230567e-06.\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6928 - acc: 0.6444 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 1.00000\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 2s 501ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 1.00000\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.6930 - acc: 0.4667 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00082: ReduceLROnPlateau reducing learning rate to 3.0223149224184457e-06.\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 1.00000\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.6930 - acc: 0.4667 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 1.00000\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 2s 513ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00085: ReduceLROnPlateau reducing learning rate to 2.4178520106943328e-06.\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 1.00000\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 1.00000\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 413ms/step - loss: 0.6929 - acc: 0.6222 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.9342816813150422e-06.\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 2s 532ms/step - loss: 0.6931 - acc: 0.4444 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 1.00000\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 1.00000\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.6928 - acc: 0.6222 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.547425381431822e-06.\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 1.00000\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 2s 530ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 1.00000\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.2379403415252455e-06.\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6929 - acc: 0.5111 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 1.00000\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 1.00000\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 2s 525ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 9.903522368404083e-07.\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 1.00000\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.6929 - acc: 0.5778 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 1.00000\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 437ms/step - loss: 0.6930 - acc: 0.4667 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00100: ReduceLROnPlateau reducing learning rate to 7.922817530925386e-07.\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 2s 558ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 1.00000\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 402ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 1.00000\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 410ms/step - loss: 0.6929 - acc: 0.5778 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00103: ReduceLROnPlateau reducing learning rate to 6.338254024740309e-07.\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.6930 - acc: 0.4222 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 1.00000\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 490ms/step - loss: 0.6929 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 1.00000\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 410ms/step - loss: 0.6929 - acc: 0.6222 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00106: ReduceLROnPlateau reducing learning rate to 5.070603037893307e-07.\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 1.00000\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6931 - acc: 0.4889 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 1.00000\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 497ms/step - loss: 0.6928 - acc: 0.6889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00109: ReduceLROnPlateau reducing learning rate to 4.056482339365175e-07.\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6928 - acc: 0.6444 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 1.00000\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.6930 - acc: 0.4444 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 1.00000\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.6931 - acc: 0.4222 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00112: ReduceLROnPlateau reducing learning rate to 3.24518578054267e-07.\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 2s 510ms/step - loss: 0.6929 - acc: 0.5778 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 1.00000\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 1.00000\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00115: ReduceLROnPlateau reducing learning rate to 2.5961485334846656e-07.\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6930 - acc: 0.4222 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 1.00000\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 2s 502ms/step - loss: 0.6929 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 1.00000\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00118: ReduceLROnPlateau reducing learning rate to 2.076918917737203e-07.\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 1.00000\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 1.00000\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 2s 522ms/step - loss: 0.6930 - acc: 0.4222 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1.6615351796644973e-07.\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.6930 - acc: 0.4444 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 1.00000\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 403ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6937 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 1.00000\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00124: ReduceLROnPlateau reducing learning rate to 1.329228098256863e-07.\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 494ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 1.00000\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 363ms/step - loss: 0.6929 - acc: 0.6000 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 1.00000\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00127: ReduceLROnPlateau reducing learning rate to 1.0633824558681227e-07.\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6930 - acc: 0.4444 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 1.00000\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 485ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 1.00000\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.6929 - acc: 0.6000 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00130: ReduceLROnPlateau reducing learning rate to 8.507059874318657e-08.\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 1.00000\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.6927 - acc: 0.7111 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 1.00000\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 2s 508ms/step - loss: 0.6930 - acc: 0.5778 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00133: ReduceLROnPlateau reducing learning rate to 6.80564767208125e-08.\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6929 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 1.00000\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 1.00000\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6930 - acc: 0.4222 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00136: ReduceLROnPlateau reducing learning rate to 5.4445183650386755e-08.\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 498ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 1.00000\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6930 - acc: 0.4444 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 1.00000\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.6928 - acc: 0.6889 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00139: ReduceLROnPlateau reducing learning rate to 4.3556147488743596e-08.\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 1.00000\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 2s 520ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 1.00000\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6928 - acc: 0.6444 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00142: ReduceLROnPlateau reducing learning rate to 3.484491912786325e-08.\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 1.00000\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 1.00000\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 2s 518ms/step - loss: 0.6930 - acc: 0.4667 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00145: ReduceLROnPlateau reducing learning rate to 2.787593587072479e-08.\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 1.00000\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6929 - acc: 0.6000 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 1.00000\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 430ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00148: ReduceLROnPlateau reducing learning rate to 2.2300748980796928e-08.\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 2s 552ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 1.00000\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 401ms/step - loss: 0.6929 - acc: 0.6222 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 1.00000\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 421ms/step - loss: 0.6930 - acc: 0.4667 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 1.784059975307173e-08.\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 1.00000\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 2s 536ms/step - loss: 0.6928 - acc: 0.6000 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 1.00000\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 410ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00154: ReduceLROnPlateau reducing learning rate to 1.4272480086674477e-08.\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 1.00000\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.6930 - acc: 0.5778 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 1.00000\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 2s 505ms/step - loss: 0.6931 - acc: 0.4444 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00157: ReduceLROnPlateau reducing learning rate to 1.1417984069339583e-08.\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.6929 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 1.00000\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6928 - acc: 0.6222 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 1.00000\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.6931 - acc: 0.4222 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 00160: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 2s 515ms/step - loss: 0.6930 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 1.00000\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 1.00000\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 363ms/step - loss: 0.6930 - acc: 0.4444 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 1.00000\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 1.00000\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 2s 521ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 1.00000\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.6930 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 1.00000\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6928 - acc: 0.6222 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 1.00000\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6929 - acc: 0.6000 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 1.00000\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 2s 525ms/step - loss: 0.6929 - acc: 0.5778 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 1.00000\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6931 - acc: 0.4667 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 1.00000\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.6928 - acc: 0.6667 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 1.00000\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6929 - acc: 0.6000 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 1.00000\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 2s 530ms/step - loss: 0.6929 - acc: 0.5778 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 1.00000\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.6930 - acc: 0.4222 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 1.00000\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 1.00000\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 1.00000\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 2s 519ms/step - loss: 0.6929 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 1.00000\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 402ms/step - loss: 0.6929 - acc: 0.6222 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 1.00000\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 1.00000\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6931 - acc: 0.4000 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 1.00000\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 2s 523ms/step - loss: 0.6930 - acc: 0.4667 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 1.00000\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 1.00000\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 1.00000\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 1.00000\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 2s 516ms/step - loss: 0.6928 - acc: 0.6222 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 1.00000\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.6930 - acc: 0.5333 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 1.00000\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.6931 - acc: 0.4444 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 1.00000\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 401ms/step - loss: 0.6929 - acc: 0.6222 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 1.00000\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 2s 524ms/step - loss: 0.6928 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 1.00000\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 415ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 1.00000\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.6929 - acc: 0.5333 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 1.00000\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6930 - acc: 0.5111 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 1.00000\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 2s 529ms/step - loss: 0.6930 - acc: 0.4889 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 1.00000\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6930 - acc: 0.4444 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 1.00000\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 413ms/step - loss: 0.6929 - acc: 0.6000 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 1.00000\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6932 - acc: 0.4000 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 1.00000\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 1s 493ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 1.00000\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6929 - acc: 0.5778 - val_loss: 0.6924 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 1.00000\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6928 - acc: 0.6667 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 1.00000\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6929 - acc: 0.5556 - val_loss: 0.6933 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 1.00000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fef200f7710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsG9iK3bv1He",
        "colab_type": "text"
      },
      "source": [
        "실제 학습의 진행..\n",
        "\n",
        "딥러닝 모델의 이미지 학습 (Training)을 진행하고,\n",
        "\n",
        "Epoch마다 Validation set에 대해 평가를 함.\n",
        "\n",
        "**CPU 버전으로는 매우 느리므로 실행 주의**\n",
        "**딥러닝 학습용 GPU가 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrWdw__Yv1Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DeepLearning.load_weights(model_directory+'PretrainedVGG.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZrDVtuJv1Hh",
        "colab_type": "text"
      },
      "source": [
        "제 연구실에서 위의 코드로 학습한 모델의 Weight를 제공해드립니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SSOdCrNlv1Hk",
        "colab_type": "code",
        "outputId": "e2bf42e8-b8dd-43bf-dff7-7b6ab5bb87ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "DeepLearning.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "zero_padding2d_14_input (Inp (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_14 (ZeroPaddi (None, 226, 226, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_15 (ZeroPaddi (None, 226, 226, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_16 (ZeroPaddi (None, 114, 114, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_17 (ZeroPaddi (None, 114, 114, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_18 (ZeroPaddi (None, 58, 58, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_19 (ZeroPaddi (None, 58, 58, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_20 (ZeroPaddi (None, 58, 58, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_21 (ZeroPaddi (None, 30, 30, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_22 (ZeroPaddi (None, 30, 30, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_23 (ZeroPaddi (None, 30, 30, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_24 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_25 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_26 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 14,715,714\n",
            "Trainable params: 14,715,714\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIqVwFD1v1Hm",
        "colab_type": "text"
      },
      "source": [
        "VGG 모델의 summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YLG6GJcv1Hn",
        "colab_type": "code",
        "outputId": "73598a55-e2da-4afe-fc9b-46d3e5fefdce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "test_pred=DeepLearning.predict_generator(TEST_GENERATOR,verbose=1, steps=4)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 227ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8tWnGzNv1Hp",
        "colab_type": "code",
        "outputId": "ec9b96d8-bc6b-46cc-e364-066faefcc113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "Akiec = [(test_pred[i][1]) for i in range(0,30)]\n",
        "Melanoma = [(test_pred[i][1]) for i in range(30,60)]\n",
        "\n",
        "for Result_Akiec, Result_Mela in zip(Akiec, Melanoma):\n",
        "    print([Result_Akiec, Result_Mela])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.010010782, 0.9438011]\n",
            "[0.09738692, 0.99587065]\n",
            "[0.18956165, 0.9936301]\n",
            "[0.03849322, 0.9005278]\n",
            "[0.043104827, 0.99314535]\n",
            "[0.16161136, 0.9131012]\n",
            "[0.10451101, 0.9974694]\n",
            "[0.048163738, 0.95566344]\n",
            "[0.010808472, 0.61346537]\n",
            "[0.0041364664, 0.644243]\n",
            "[0.012942117, 0.8873491]\n",
            "[0.014121742, 0.9467814]\n",
            "[0.09058328, 0.9002652]\n",
            "[0.026563067, 0.34277982]\n",
            "[0.4289438, 0.9785095]\n",
            "[0.2007183, 0.99846077]\n",
            "[0.14629695, 0.96562696]\n",
            "[0.029689996, 0.96318716]\n",
            "[0.004120451, 0.79554874]\n",
            "[0.020713726, 0.99116194]\n",
            "[0.028662862, 0.99124384]\n",
            "[0.045290355, 0.99523073]\n",
            "[0.2803001, 0.96848005]\n",
            "[0.37904364, 0.9978695]\n",
            "[0.035114948, 0.993612]\n",
            "[0.0048168013, 0.9997212]\n",
            "[0.010178826, 0.73693013]\n",
            "[0.103977725, 0.9971119]\n",
            "[0.036909115, 0.98185825]\n",
            "[0.011131993, 0.92938286]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_9V1Jzv1Hr",
        "colab_type": "text"
      },
      "source": [
        "모델 예측 결과.\n",
        "\n",
        "- 좌측: Akiec\n",
        "- 우측: Melanoma\n",
        "\n",
        "Akiec은 0에 가깝게, Melanoma는 1에 가깝게 잘 예측."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ki4JIOwGv1Hr",
        "colab_type": "code",
        "outputId": "5a0d6aeb-6d49-4408-8f3d-70a0a29d3c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "Labels = np.array([0, 1])\n",
        "y_true = np.repeat(Labels, [30, 30], axis=0)\n",
        "pred = test_pred[:,0]\n",
        "\n",
        "skplt.metrics.plot_roc_curve(y_true, test_pred)\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_roc_curve is deprecated; This will be removed in v0.5.0. Please use scikitplot.metrics.plot_roc instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywJEwqHiv1Hu",
        "colab_type": "text"
      },
      "source": [
        "그려진 ROC Curve.\n",
        "\n",
        "0은 Akiec, 1은 Melanoma를 의미.\n",
        "\n",
        "Skin classification은 딥러닝이 해결할 수 있는 매우 좋은 예제로,\n",
        "\n",
        "성능이 아주 좋은것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_MJBxJRv1Hu",
        "colab_type": "code",
        "outputId": "40841814-3ab6-4069-eb63-391dfa28183d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "Files = os.listdir(test_directory+'melanoma')\n",
        "for i in range(len(Files)):\n",
        "\timg_path = CurrentDirectory+\"TEST/melanoma/\"+Files[i]\n",
        "\toutput_path = CurrentDirectory + 'Heatmap_' + Files[i]\n",
        "\n",
        "\timg = image.load_img(img_path, target_size=(224, 224))\n",
        "\tx = image.img_to_array(img)\n",
        "\tx = np.expand_dims(x, axis=0)\n",
        "\n",
        "\tx = preprocess_input(x)\n",
        "\tpreds = DeepLearning.predict(x)\n",
        "\n",
        "\targmax = np.argmax(preds[0])\n",
        "\toutput = DeepLearning.output[:, argmax]\n",
        "\tlast_conv_layer = DeepLearning.get_layer('conv2d_26')\n",
        "\n",
        "\tgrads = K.gradients(output, last_conv_layer.output)[0]\n",
        "\tpooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\titerate = K.function([DeepLearning.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "\tpooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "\n",
        "\tfor i in range(512):\n",
        "\t\tconv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\theatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "\theatmap = np.maximum(heatmap, 0)\n",
        "\theatmap /= np.max(heatmap)\n",
        "\n",
        "\timg = cv2.imread(img_path)\n",
        "\theatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "\theatmap = np.uint8(255 * heatmap)\n",
        "\theatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\thif = .8\n",
        "\tsuperimposed_img = heatmap * hif+ img\n",
        "\n",
        "\tax = plt.subplot(1,2,1)\n",
        "\tax.imshow(img)\n",
        "\tax = plt.subplot(1,2,2)\n",
        "\tax.imshow(img)\n",
        "\tax.imshow(255-heatmap, cmap=plt.cm.jet, alpha=0.5, interpolation='nearest')\n",
        "\tplt.savefig(output_path, bbox_inches = 'tight')\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACVCAYAAACjO7rDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyH8BBHCMJUSNT/YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyieHx5lv1Hw",
        "colab_type": "text"
      },
      "source": [
        "그려진 Heatmap은 빨갛게 그려진 부분이 예측할때 주요하게 사용된 Feature map의 GAP 결과라고 해석할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bP6BCDpv1Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}